{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_wave_multi_output.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsm6jCoXJSum",
        "outputId": "5a7a4b2b-82e6-45ce-adbc-0d3ecb8096b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a51f7778"
      },
      "source": [
        "# builtins\n",
        "import locale\n",
        "import math\n",
        "import glob\n",
        "import pathlib\n",
        "import functools\n",
        "import logging\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# numerical stuff\n",
        "#import pickle5 as pickle\n",
        "import tables\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Conv2DTranspose, Reshape, Lambda\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Input, UpSampling2D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "#from tensorflow.python import ipu\n",
        "\n",
        "#import libpvti as pvti\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmKbYE3o1TQL",
        "outputId": "ece3ea43-d9f0-4a39-c932-f88b5f14f8df"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.64.129.218:8470']\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.64.129.218:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.64.129.218:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ed43b62"
      },
      "source": [
        "data_path_train = 'gs://bathy_sample/processed/20211013/train_data_mask_no_schematic'\n",
        "data_path_test = 'gs://bathy_sample/processed/20211013/test_data_mask_no_schematic'\n",
        "#meta_data_path = '/mnt/poddata/data/bathy-emodnet-a-runs.h5'\n",
        "#all_checkpoints_path = 'gs://bathy_sample/dnn/checkpoints'\n",
        "all_checkpoints_path = '/content/drive/MyDrive/DeepLearning/Benchmark/checkpoints'\n",
        "model_name = 'guus-2d-mlp-cnn-v0.4'\n",
        "model_path = '/content/drive/MyDrive/DeepLearning/Benchmark/mask_models'\n",
        "checkpoints_path = all_checkpoints_path + '/' + model_name\n",
        "\n",
        "learning_rate = 1e-4\n",
        "n_epochs = 100\n",
        "batch_size = 8 * tpu_strategy.num_replicas_in_sync\n",
        "steps_per_execution = 1\n",
        "steps_per_epoch = 112\n",
        "validation_steps = 320\n",
        "gradient_accumulation_steps_per_replica = 8\n",
        "raster_shape = (256, 256, 1)\n",
        "input_attributes = np.load('/content/drive/MyDrive/DeepLearning/input_attributes.npy', allow_pickle=True).item()\n",
        "(eta_mean, eta_std) = input_attributes['eta'].values()\n",
        "(zeta_mean, zeta_std) = input_attributes['zeta'].values()\n",
        "(bathy_mean, bathy_std) = input_attributes['bathy'].values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eaa8178"
      },
      "source": [
        "def tf_parse(eg):\n",
        "    \"\"\"parse an example (or batch of examples, not quite sure...)\"\"\"\n",
        "\n",
        "    # here we re-specify our format\n",
        "    # you can also infer the format from the data using tf.train.Example.FromString\n",
        "    # but that did not work\n",
        "    example = tf.io.parse_example(\n",
        "        eg[tf.newaxis],\n",
        "        {\n",
        "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'bathy': tf.io.FixedLenFeature([], tf.string),\n",
        "            'hs': tf.io.FixedLenFeature([], tf.string),\n",
        "            'tm01': tf.io.FixedLenFeature([], tf.string),\n",
        "            'theta0x': tf.io.FixedLenFeature([], tf.string),\n",
        "            'theta0y': tf.io.FixedLenFeature([], tf.string),\n",
        "            'eta': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'zeta': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'theta_wavex': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'theta_wavey': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'mask': tf.io.FixedLenFeature([], tf.string),\n",
        "        },\n",
        "    )\n",
        "    bathy = tf.io.parse_tensor(example[\"bathy\"][0], out_type=\"float32\")\n",
        "    bathy = tf.ensure_shape(bathy, raster_shape)    # ensure shape, to be able to train the model\n",
        "    hs = tf.io.parse_tensor(example[\"hs\"][0], out_type=\"float32\")\n",
        "    hs = tf.ensure_shape(hs, raster_shape)\n",
        "    tm01 = tf.io.parse_tensor(example[\"tm01\"][0], out_type=\"float32\")\n",
        "    tm01 = tf.ensure_shape(tm01, raster_shape)\n",
        "    theta0x = tf.io.parse_tensor(example[\"theta0x\"][0], out_type=\"float32\")\n",
        "    theta0x = tf.ensure_shape(theta0x, raster_shape)\n",
        "    theta0y = tf.io.parse_tensor(example[\"theta0y\"][0], out_type=\"float32\")\n",
        "    theta0y = tf.ensure_shape(theta0y, raster_shape)\n",
        "    eta = example[\"eta\"]\n",
        "    zeta = example[\"zeta\"]\n",
        "    theta_wavex = example[\"theta_wavex\"]\n",
        "    theta_wavey = example[\"theta_wavey\"]\n",
        "    # Angles were in degrees instead of radians when dataset created, so adjust theta_wavex and theta_wavey\n",
        "    theta_wave = (tf.math.atan2(theta_wavey, theta_wavex) + 2*np.pi) % (2*np.pi) * np.pi / 180.\n",
        "    theta_wavex = tf.math.cos(theta_wave)\n",
        "    theta_wavey = tf.math.sin(theta_wave)\n",
        "    # Output mask\n",
        "    mask = tf.io.parse_tensor(example[\"mask\"][0], out_type=\"bool\")\n",
        "    mask = tf.cast(mask, dtype=\"int16\")\n",
        "    mask = tf.ensure_shape(mask, raster_shape)\n",
        "    # Create mask for input where waterlevel is less than bathymetry\n",
        "    bathy_mask = tf.math.greater(bathy * bathy_std + bathy_mean, zeta * zeta_std + zeta_mean)\n",
        "    bathy_mask = tf.cast(bathy_mask, dtype=\"float32\")\n",
        "    img_input = tf.concat([bathy,bathy_mask],-1)\n",
        "    attr = tf.stack([eta, zeta, theta_wavex, theta_wavey], axis=1)\n",
        "    attr = tf.reshape(attr,shape=[-1])\n",
        "    output = (hs, tm01, theta0x, theta0y)\n",
        "    output = tf.concat([hs, tm01, theta0x, theta0y], axis=-1)\n",
        "    #output = hs\n",
        "    return (img_input, attr), (output, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85f602af"
      },
      "source": [
        "def get_files(data_path):\n",
        "    files = tf.io.gfile.glob(data_path + \"/\" + \"*.tfrecords\")\n",
        "    return files\n",
        "\n",
        "def get_dataset(files):\n",
        "    \"\"\"return a tfrecord dataset with all tfrecord files\"\"\"\n",
        "    dataset =  tf.data.TFRecordDataset(files)\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9OjbHWFSu4s"
      },
      "source": [
        "def AttnBlock2D(x, g, inter_channel):\n",
        "\n",
        "    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1])(x)\n",
        "\n",
        "    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1])(g)\n",
        "\n",
        "    f = Activation('relu')(tf.keras.layers.add([theta_x, phi_g]))\n",
        "\n",
        "    psi_f = Conv2D(1, [1, 1], strides=[1, 1])(f)\n",
        "\n",
        "    rate = Activation('sigmoid')(psi_f)\n",
        "\n",
        "    att_x = tf.keras.layers.multiply([x, rate])\n",
        "\n",
        "    return att_x\n",
        "\n",
        "\n",
        "def attention_up_and_concate(down_layer, layer):\n",
        "    \n",
        "    in_channel = down_layer.get_shape().as_list()[3]\n",
        "\n",
        "    up = UpSampling2D(size=(2, 2))(down_layer)\n",
        "    layer = AttnBlock2D(x=layer, g=up, inter_channel=in_channel // 4)\n",
        "    \n",
        "    my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
        "    \n",
        "    concate = my_concat([up, layer])\n",
        "    return concate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47f5c590"
      },
      "source": [
        "def full_model(cnn_input_shape, mlp_input_shape):\n",
        "    \n",
        "    mlp_input = Input(mlp_input_shape)\n",
        "    cnn_input = Input(cnn_input_shape)\n",
        "    x = Dense(256, activation='relu')(mlp_input)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    mlp_output = Dense(4, activation='relu')(x)\n",
        "\n",
        "    x = Conv2D(16, (3,3), padding=\"same\")(cnn_input)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(32, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Downsampling with stride=2\n",
        "    x = Conv2D(64, (3,3), padding=\"same\", strides=(2,2))(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(128, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    cnn_output = BatchNormalization()(x)\n",
        "\n",
        "    conv_shape = K.int_shape(cnn_output)\n",
        "\n",
        "    # Reshape output from MLP to CNN shape to concatenate\n",
        "    x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation=\"relu\")(mlp_output)\n",
        "    x = Reshape((conv_shape[1],conv_shape[2],int(conv_shape[3])))(x)\n",
        "    \n",
        "    last_x = Concatenate()([x,cnn_output])\n",
        "\n",
        "    #################################### Normal output\n",
        "\n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(last_x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Upsampling with stride=2    \n",
        "    x = Conv2DTranspose(128, (3,3), padding=\"same\", strides=(2,2))(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(32, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(16, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "  \n",
        "    normal_output = Conv2DTranspose(4, (3,3), padding=\"same\", activation=\"linear\", name=\"normal_output\")(x)\n",
        "  \n",
        "    ############################### Mask output\n",
        "\n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(last_x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    # Upsampling with stride=2\n",
        "    x = Conv2DTranspose(128, (3,3), padding=\"same\", strides=(2,2))(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(32, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(16, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    mask_output = Conv2DTranspose(1, (3,3), padding=\"same\", activation=\"sigmoid\", name=\"mask_output\")(x)\n",
        "\n",
        "    model = Model(inputs=[cnn_input, mlp_input], outputs = [normal_output, mask_output])\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c585d94"
      },
      "source": [
        "train_files = get_files(data_path_train)\n",
        "test_files = get_files(data_path_test)\n",
        "\n",
        "train_dataset = get_dataset(train_files)\n",
        "test_dataset = get_dataset(test_files)\n",
        "\n",
        "#Filter out the bathymetries with heights greater than 100m\n",
        "def filter_fn(x, y):\n",
        "    return tf.less_equal(tf.reduce_max(x[0][:,:,0]), (100-bathy_mean)/bathy_std)\n",
        "\n",
        "train_dataset = train_dataset.filter(filter_fn)\n",
        "test_dataset = test_dataset.filter(filter_fn)\n",
        "\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5701e49"
      },
      "source": [
        "#for sample in train_dataset.take(1):\n",
        "#    print(repr(sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a01a94a",
        "outputId": "73df82d6-1790-4eed-e4c9-3a1f04476a07"
      },
      "source": [
        "start = time.time()\n",
        "print(time.ctime(start))\n",
        "with tpu_strategy.scope():\n",
        "    model = full_model((256, 256, 2), 4)\n",
        "    opt = Adam(learning_rate=learning_rate, decay=learning_rate / n_epochs)\n",
        "\n",
        "    losses = {\n",
        "\t  \"normal_output\": \"mean_squared_error\",\n",
        "  \t\"mask_output\": \"binary_crossentropy\",\n",
        "    }\n",
        "    lossWeights = {\"normal_output\": 1.0, \"mask_output\": 1.0}\n",
        "\n",
        "    model.compile(loss=losses, optimizer=opt, steps_per_execution=steps_per_execution)\n",
        "    \n",
        "    callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoints_path, \n",
        "        save_weights_only=True,\n",
        "        #monitor='val_mse',\n",
        "        mode='max',\n",
        "        save_best_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.CSVLogger(\n",
        "        filename=checkpoints_path + '.csv')\n",
        "    ]\n",
        "    \n",
        "    model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs)#, callbacks=callbacks)#, steps_per_epoch=steps_per_epoch)\n",
        "    #model.save(model_path + '/' + model_name + '.h5')\n",
        "\n",
        "end = time.time()\n",
        "print(time.ctime(end))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 24 13:11:04 2021\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 574s 6s/step - loss: 54.0266 - normal_output_loss: 53.3551 - mask_output_loss: 0.6715 - val_loss: 53.1851 - val_normal_output_loss: 52.5577 - val_mask_output_loss: 0.6275\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 50.5701 - normal_output_loss: 49.9521 - mask_output_loss: 0.6180 - val_loss: 40.8876 - val_normal_output_loss: 40.3828 - val_mask_output_loss: 0.5048\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 47.3462 - normal_output_loss: 46.8170 - mask_output_loss: 0.5292 - val_loss: 37.8303 - val_normal_output_loss: 37.4151 - val_mask_output_loss: 0.4152\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 45.2666 - normal_output_loss: 44.8281 - mask_output_loss: 0.4385 - val_loss: 35.6726 - val_normal_output_loss: 35.3663 - val_mask_output_loss: 0.3063\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 61s 658ms/step - loss: 42.9808 - normal_output_loss: 42.6183 - mask_output_loss: 0.3625 - val_loss: 34.2856 - val_normal_output_loss: 34.0450 - val_mask_output_loss: 0.2406\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 60s 653ms/step - loss: 40.4145 - normal_output_loss: 40.1175 - mask_output_loss: 0.2970 - val_loss: 33.4233 - val_normal_output_loss: 33.2281 - val_mask_output_loss: 0.1952\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 37.5738 - normal_output_loss: 37.3303 - mask_output_loss: 0.2435 - val_loss: 32.9664 - val_normal_output_loss: 32.8036 - val_mask_output_loss: 0.1628\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 59s 643ms/step - loss: 34.4762 - normal_output_loss: 34.2761 - mask_output_loss: 0.2001 - val_loss: 31.5864 - val_normal_output_loss: 31.4561 - val_mask_output_loss: 0.1303\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 31.2085 - normal_output_loss: 31.0382 - mask_output_loss: 0.1703 - val_loss: 28.3041 - val_normal_output_loss: 28.2036 - val_mask_output_loss: 0.1005\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 27.8886 - normal_output_loss: 27.7500 - mask_output_loss: 0.1386 - val_loss: 24.5794 - val_normal_output_loss: 24.4944 - val_mask_output_loss: 0.0850\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 24.5887 - normal_output_loss: 24.4784 - mask_output_loss: 0.1103 - val_loss: 21.3895 - val_normal_output_loss: 21.3172 - val_mask_output_loss: 0.0724\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 21.4131 - normal_output_loss: 21.3186 - mask_output_loss: 0.0946 - val_loss: 18.9649 - val_normal_output_loss: 18.8950 - val_mask_output_loss: 0.0700\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 18.4104 - normal_output_loss: 18.3327 - mask_output_loss: 0.0778 - val_loss: 15.9479 - val_normal_output_loss: 15.8862 - val_mask_output_loss: 0.0617\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 15.6473 - normal_output_loss: 15.5824 - mask_output_loss: 0.0649 - val_loss: 13.5219 - val_normal_output_loss: 13.4743 - val_mask_output_loss: 0.0476\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 60s 643ms/step - loss: 13.1668 - normal_output_loss: 13.1096 - mask_output_loss: 0.0572 - val_loss: 11.3109 - val_normal_output_loss: 11.2469 - val_mask_output_loss: 0.0640\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 10.9264 - normal_output_loss: 10.8749 - mask_output_loss: 0.0515 - val_loss: 9.1439 - val_normal_output_loss: 9.0935 - val_mask_output_loss: 0.0503\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 8.9868 - normal_output_loss: 8.9425 - mask_output_loss: 0.0443 - val_loss: 7.4998 - val_normal_output_loss: 7.4652 - val_mask_output_loss: 0.0346\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 7.3326 - normal_output_loss: 7.2927 - mask_output_loss: 0.0399 - val_loss: 5.9610 - val_normal_output_loss: 5.9244 - val_mask_output_loss: 0.0366\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 5.9680 - normal_output_loss: 5.9305 - mask_output_loss: 0.0375 - val_loss: 4.9595 - val_normal_output_loss: 4.9256 - val_mask_output_loss: 0.0339\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 60s 651ms/step - loss: 4.8307 - normal_output_loss: 4.7976 - mask_output_loss: 0.0331 - val_loss: 3.9544 - val_normal_output_loss: 3.9193 - val_mask_output_loss: 0.0351\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 3.9326 - normal_output_loss: 3.9013 - mask_output_loss: 0.0313 - val_loss: 2.9426 - val_normal_output_loss: 2.9133 - val_mask_output_loss: 0.0293\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 3.1823 - normal_output_loss: 3.1557 - mask_output_loss: 0.0265 - val_loss: 2.2217 - val_normal_output_loss: 2.1951 - val_mask_output_loss: 0.0266\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 60s 643ms/step - loss: 2.6345 - normal_output_loss: 2.6092 - mask_output_loss: 0.0253 - val_loss: 1.7824 - val_normal_output_loss: 1.7611 - val_mask_output_loss: 0.0213\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 59s 634ms/step - loss: 2.2142 - normal_output_loss: 2.1889 - mask_output_loss: 0.0253 - val_loss: 1.3870 - val_normal_output_loss: 1.3655 - val_mask_output_loss: 0.0216\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 58s 627ms/step - loss: 1.8802 - normal_output_loss: 1.8572 - mask_output_loss: 0.0230 - val_loss: 1.1344 - val_normal_output_loss: 1.1108 - val_mask_output_loss: 0.0236\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 1.6328 - normal_output_loss: 1.6110 - mask_output_loss: 0.0218 - val_loss: 0.8350 - val_normal_output_loss: 0.8157 - val_mask_output_loss: 0.0193\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 1.4626 - normal_output_loss: 1.4414 - mask_output_loss: 0.0212 - val_loss: 0.7614 - val_normal_output_loss: 0.7381 - val_mask_output_loss: 0.0233\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 60s 646ms/step - loss: 1.3299 - normal_output_loss: 1.3093 - mask_output_loss: 0.0206 - val_loss: 0.5881 - val_normal_output_loss: 0.5709 - val_mask_output_loss: 0.0171\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 59s 642ms/step - loss: 1.2053 - normal_output_loss: 1.1870 - mask_output_loss: 0.0183 - val_loss: 0.5731 - val_normal_output_loss: 0.5538 - val_mask_output_loss: 0.0193\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 1.1574 - normal_output_loss: 1.1381 - mask_output_loss: 0.0194 - val_loss: 0.5434 - val_normal_output_loss: 0.5244 - val_mask_output_loss: 0.0190\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 59s 642ms/step - loss: 1.0873 - normal_output_loss: 1.0701 - mask_output_loss: 0.0171 - val_loss: 0.6094 - val_normal_output_loss: 0.5773 - val_mask_output_loss: 0.0321\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 1.0538 - normal_output_loss: 1.0368 - mask_output_loss: 0.0171 - val_loss: 0.5074 - val_normal_output_loss: 0.4878 - val_mask_output_loss: 0.0196\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 59s 638ms/step - loss: 1.0527 - normal_output_loss: 1.0354 - mask_output_loss: 0.0173 - val_loss: 0.4148 - val_normal_output_loss: 0.3984 - val_mask_output_loss: 0.0164\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 1.0238 - normal_output_loss: 1.0080 - mask_output_loss: 0.0158 - val_loss: 0.4740 - val_normal_output_loss: 0.4568 - val_mask_output_loss: 0.0172\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 60s 647ms/step - loss: 1.0100 - normal_output_loss: 0.9951 - mask_output_loss: 0.0148 - val_loss: 0.4114 - val_normal_output_loss: 0.3936 - val_mask_output_loss: 0.0179\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 1.0133 - normal_output_loss: 0.9979 - mask_output_loss: 0.0155 - val_loss: 0.4148 - val_normal_output_loss: 0.3990 - val_mask_output_loss: 0.0159\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.9948 - normal_output_loss: 0.9800 - mask_output_loss: 0.0148 - val_loss: 0.4087 - val_normal_output_loss: 0.3931 - val_mask_output_loss: 0.0156\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.9786 - normal_output_loss: 0.9651 - mask_output_loss: 0.0135 - val_loss: 0.3762 - val_normal_output_loss: 0.3623 - val_mask_output_loss: 0.0139\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 59s 642ms/step - loss: 0.9610 - normal_output_loss: 0.9486 - mask_output_loss: 0.0125 - val_loss: 0.4048 - val_normal_output_loss: 0.3881 - val_mask_output_loss: 0.0168\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 0.9593 - normal_output_loss: 0.9470 - mask_output_loss: 0.0123 - val_loss: 0.4271 - val_normal_output_loss: 0.4123 - val_mask_output_loss: 0.0148\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 0.9585 - normal_output_loss: 0.9461 - mask_output_loss: 0.0124 - val_loss: 0.4093 - val_normal_output_loss: 0.3921 - val_mask_output_loss: 0.0172\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 59s 642ms/step - loss: 0.9401 - normal_output_loss: 0.9292 - mask_output_loss: 0.0109 - val_loss: 0.4001 - val_normal_output_loss: 0.3833 - val_mask_output_loss: 0.0168\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 0.9544 - normal_output_loss: 0.9423 - mask_output_loss: 0.0121 - val_loss: 0.3901 - val_normal_output_loss: 0.3767 - val_mask_output_loss: 0.0134\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 0.9429 - normal_output_loss: 0.9317 - mask_output_loss: 0.0112 - val_loss: 0.4058 - val_normal_output_loss: 0.3908 - val_mask_output_loss: 0.0150\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.9438 - normal_output_loss: 0.9320 - mask_output_loss: 0.0118 - val_loss: 0.4219 - val_normal_output_loss: 0.4069 - val_mask_output_loss: 0.0150\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 59s 634ms/step - loss: 0.9416 - normal_output_loss: 0.9304 - mask_output_loss: 0.0112 - val_loss: 0.3628 - val_normal_output_loss: 0.3502 - val_mask_output_loss: 0.0126\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 58s 623ms/step - loss: 0.9297 - normal_output_loss: 0.9193 - mask_output_loss: 0.0104 - val_loss: 0.3626 - val_normal_output_loss: 0.3488 - val_mask_output_loss: 0.0138\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 0.9494 - normal_output_loss: 0.9359 - mask_output_loss: 0.0134 - val_loss: 0.4133 - val_normal_output_loss: 0.3978 - val_mask_output_loss: 0.0155\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 61s 655ms/step - loss: 0.9226 - normal_output_loss: 0.9112 - mask_output_loss: 0.0114 - val_loss: 0.3640 - val_normal_output_loss: 0.3510 - val_mask_output_loss: 0.0129\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 60s 646ms/step - loss: 0.9237 - normal_output_loss: 0.9128 - mask_output_loss: 0.0109 - val_loss: 0.3994 - val_normal_output_loss: 0.3836 - val_mask_output_loss: 0.0158\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 59s 642ms/step - loss: 0.9285 - normal_output_loss: 0.9171 - mask_output_loss: 0.0114 - val_loss: 0.4428 - val_normal_output_loss: 0.4230 - val_mask_output_loss: 0.0199\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 0.9466 - normal_output_loss: 0.9336 - mask_output_loss: 0.0130 - val_loss: 0.4383 - val_normal_output_loss: 0.4093 - val_mask_output_loss: 0.0290\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 58s 636ms/step - loss: 0.9296 - normal_output_loss: 0.9161 - mask_output_loss: 0.0135 - val_loss: 0.4734 - val_normal_output_loss: 0.4548 - val_mask_output_loss: 0.0186\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 59s 633ms/step - loss: 0.9146 - normal_output_loss: 0.9031 - mask_output_loss: 0.0115 - val_loss: 0.4376 - val_normal_output_loss: 0.4227 - val_mask_output_loss: 0.0149\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 58s 623ms/step - loss: 0.8866 - normal_output_loss: 0.8764 - mask_output_loss: 0.0103 - val_loss: 0.4320 - val_normal_output_loss: 0.4148 - val_mask_output_loss: 0.0172\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 58s 627ms/step - loss: 0.8880 - normal_output_loss: 0.8773 - mask_output_loss: 0.0107 - val_loss: 0.5519 - val_normal_output_loss: 0.5269 - val_mask_output_loss: 0.0249\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 0.8837 - normal_output_loss: 0.8721 - mask_output_loss: 0.0116 - val_loss: 0.5779 - val_normal_output_loss: 0.5486 - val_mask_output_loss: 0.0293\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 57s 621ms/step - loss: 0.8656 - normal_output_loss: 0.8558 - mask_output_loss: 0.0098 - val_loss: 0.4959 - val_normal_output_loss: 0.4726 - val_mask_output_loss: 0.0233\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 57s 611ms/step - loss: 0.8514 - normal_output_loss: 0.8420 - mask_output_loss: 0.0095 - val_loss: 0.5100 - val_normal_output_loss: 0.4888 - val_mask_output_loss: 0.0212\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 58s 628ms/step - loss: 0.8335 - normal_output_loss: 0.8248 - mask_output_loss: 0.0087 - val_loss: 0.4848 - val_normal_output_loss: 0.4656 - val_mask_output_loss: 0.0192\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 58s 627ms/step - loss: 0.8217 - normal_output_loss: 0.8134 - mask_output_loss: 0.0083 - val_loss: 0.4629 - val_normal_output_loss: 0.4443 - val_mask_output_loss: 0.0186\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 65s 709ms/step - loss: 0.8434 - normal_output_loss: 0.8332 - mask_output_loss: 0.0102 - val_loss: 0.4183 - val_normal_output_loss: 0.4003 - val_mask_output_loss: 0.0180\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 59s 633ms/step - loss: 0.8093 - normal_output_loss: 0.8007 - mask_output_loss: 0.0085 - val_loss: 0.4103 - val_normal_output_loss: 0.3933 - val_mask_output_loss: 0.0170\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 57s 616ms/step - loss: 0.8131 - normal_output_loss: 0.8040 - mask_output_loss: 0.0090 - val_loss: 0.4479 - val_normal_output_loss: 0.4309 - val_mask_output_loss: 0.0171\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 58s 629ms/step - loss: 0.8072 - normal_output_loss: 0.7986 - mask_output_loss: 0.0087 - val_loss: 0.4618 - val_normal_output_loss: 0.4457 - val_mask_output_loss: 0.0162\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.8099 - normal_output_loss: 0.8008 - mask_output_loss: 0.0091 - val_loss: 0.4456 - val_normal_output_loss: 0.4230 - val_mask_output_loss: 0.0226\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 0.8123 - normal_output_loss: 0.8029 - mask_output_loss: 0.0094 - val_loss: 0.4384 - val_normal_output_loss: 0.4223 - val_mask_output_loss: 0.0161\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 63s 684ms/step - loss: 0.7931 - normal_output_loss: 0.7841 - mask_output_loss: 0.0090 - val_loss: 0.4854 - val_normal_output_loss: 0.4696 - val_mask_output_loss: 0.0158\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 0.7836 - normal_output_loss: 0.7749 - mask_output_loss: 0.0087 - val_loss: 0.5174 - val_normal_output_loss: 0.4939 - val_mask_output_loss: 0.0235\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 59s 633ms/step - loss: 0.7926 - normal_output_loss: 0.7827 - mask_output_loss: 0.0099 - val_loss: 0.4897 - val_normal_output_loss: 0.4595 - val_mask_output_loss: 0.0301\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.7790 - normal_output_loss: 0.7699 - mask_output_loss: 0.0092 - val_loss: 0.5414 - val_normal_output_loss: 0.5128 - val_mask_output_loss: 0.0286\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 0.7877 - normal_output_loss: 0.7775 - mask_output_loss: 0.0102 - val_loss: 0.4731 - val_normal_output_loss: 0.4516 - val_mask_output_loss: 0.0216\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 58s 626ms/step - loss: 0.7753 - normal_output_loss: 0.7665 - mask_output_loss: 0.0088 - val_loss: 0.4536 - val_normal_output_loss: 0.4321 - val_mask_output_loss: 0.0216\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 0.7571 - normal_output_loss: 0.7492 - mask_output_loss: 0.0079 - val_loss: 0.4762 - val_normal_output_loss: 0.4518 - val_mask_output_loss: 0.0244\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.7491 - normal_output_loss: 0.7413 - mask_output_loss: 0.0078 - val_loss: 0.4536 - val_normal_output_loss: 0.4328 - val_mask_output_loss: 0.0208\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.7565 - normal_output_loss: 0.7481 - mask_output_loss: 0.0084 - val_loss: 0.5281 - val_normal_output_loss: 0.4958 - val_mask_output_loss: 0.0322\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 58s 632ms/step - loss: 0.7483 - normal_output_loss: 0.7397 - mask_output_loss: 0.0086 - val_loss: 0.5034 - val_normal_output_loss: 0.4783 - val_mask_output_loss: 0.0251\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.7448 - normal_output_loss: 0.7359 - mask_output_loss: 0.0088 - val_loss: 0.5516 - val_normal_output_loss: 0.5272 - val_mask_output_loss: 0.0244\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 58s 633ms/step - loss: 0.7448 - normal_output_loss: 0.7361 - mask_output_loss: 0.0087 - val_loss: 0.5575 - val_normal_output_loss: 0.5299 - val_mask_output_loss: 0.0276\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.7160 - normal_output_loss: 0.7084 - mask_output_loss: 0.0076 - val_loss: 0.4788 - val_normal_output_loss: 0.4584 - val_mask_output_loss: 0.0205\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 0.7084 - normal_output_loss: 0.7006 - mask_output_loss: 0.0077 - val_loss: 0.5382 - val_normal_output_loss: 0.5160 - val_mask_output_loss: 0.0222\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 0.7078 - normal_output_loss: 0.7000 - mask_output_loss: 0.0078 - val_loss: 0.5056 - val_normal_output_loss: 0.4844 - val_mask_output_loss: 0.0211\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 58s 627ms/step - loss: 0.7012 - normal_output_loss: 0.6935 - mask_output_loss: 0.0076 - val_loss: 0.5074 - val_normal_output_loss: 0.4904 - val_mask_output_loss: 0.0169\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 59s 638ms/step - loss: 0.7110 - normal_output_loss: 0.7030 - mask_output_loss: 0.0080 - val_loss: 0.5204 - val_normal_output_loss: 0.4988 - val_mask_output_loss: 0.0216\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 0.7093 - normal_output_loss: 0.7013 - mask_output_loss: 0.0080 - val_loss: 0.4735 - val_normal_output_loss: 0.4592 - val_mask_output_loss: 0.0144\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 0.7057 - normal_output_loss: 0.6972 - mask_output_loss: 0.0086 - val_loss: 0.4731 - val_normal_output_loss: 0.4530 - val_mask_output_loss: 0.0201\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 58s 629ms/step - loss: 0.7145 - normal_output_loss: 0.7048 - mask_output_loss: 0.0096 - val_loss: 0.5013 - val_normal_output_loss: 0.4793 - val_mask_output_loss: 0.0220\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.6895 - normal_output_loss: 0.6815 - mask_output_loss: 0.0080 - val_loss: 0.4826 - val_normal_output_loss: 0.4595 - val_mask_output_loss: 0.0231\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 58s 628ms/step - loss: 0.6701 - normal_output_loss: 0.6632 - mask_output_loss: 0.0069 - val_loss: 0.4905 - val_normal_output_loss: 0.4677 - val_mask_output_loss: 0.0228\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 0.6590 - normal_output_loss: 0.6530 - mask_output_loss: 0.0060 - val_loss: 0.5694 - val_normal_output_loss: 0.5390 - val_mask_output_loss: 0.0304\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 0.6666 - normal_output_loss: 0.6596 - mask_output_loss: 0.0071 - val_loss: 0.4896 - val_normal_output_loss: 0.4755 - val_mask_output_loss: 0.0140\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.6600 - normal_output_loss: 0.6527 - mask_output_loss: 0.0073 - val_loss: 0.5097 - val_normal_output_loss: 0.4846 - val_mask_output_loss: 0.0251\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 58s 628ms/step - loss: 0.6439 - normal_output_loss: 0.6374 - mask_output_loss: 0.0064 - val_loss: 0.5196 - val_normal_output_loss: 0.4938 - val_mask_output_loss: 0.0258\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 0.6396 - normal_output_loss: 0.6331 - mask_output_loss: 0.0065 - val_loss: 0.5571 - val_normal_output_loss: 0.5254 - val_mask_output_loss: 0.0318\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.6463 - normal_output_loss: 0.6391 - mask_output_loss: 0.0071 - val_loss: 0.5434 - val_normal_output_loss: 0.5232 - val_mask_output_loss: 0.0202\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 59s 633ms/step - loss: 0.6485 - normal_output_loss: 0.6415 - mask_output_loss: 0.0069 - val_loss: 0.5470 - val_normal_output_loss: 0.5227 - val_mask_output_loss: 0.0243\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 58s 628ms/step - loss: 0.6316 - normal_output_loss: 0.6255 - mask_output_loss: 0.0061 - val_loss: 0.6055 - val_normal_output_loss: 0.5679 - val_mask_output_loss: 0.0376\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 58s 626ms/step - loss: 0.6383 - normal_output_loss: 0.6319 - mask_output_loss: 0.0064 - val_loss: 0.4713 - val_normal_output_loss: 0.4510 - val_mask_output_loss: 0.0203\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 58s 627ms/step - loss: 0.6444 - normal_output_loss: 0.6378 - mask_output_loss: 0.0066 - val_loss: 0.5417 - val_normal_output_loss: 0.5180 - val_mask_output_loss: 0.0237\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.6352 - normal_output_loss: 0.6286 - mask_output_loss: 0.0066 - val_loss: 0.4853 - val_normal_output_loss: 0.4685 - val_mask_output_loss: 0.0168\n",
            "Wed Nov 24 14:59:32 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dbtGZEzmKqXL",
        "outputId": "69523b42-eeff-4a8b-c499-9a608c19ba23"
      },
      "source": [
        "model = full_model((256, 256, 1), 4)\n",
        "opt = Adam(learning_rate=learning_rate, decay=learning_rate / n_epochs)\n",
        "\n",
        "losses = {\n",
        "\t\"normal_output\": \"mean_squared_error\",\n",
        "\t\"mask_output\": \"binary_crossentropy\",\n",
        "}\n",
        "lossWeights = {\"normal_output\": 1.0, \"mask_output\": 1.0}\n",
        "\n",
        "model.compile(loss=losses, optimizer=opt, steps_per_execution=steps_per_execution)\n",
        "\n",
        "callbacks = [\n",
        "tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoints_path, \n",
        "    save_weights_only=True,\n",
        "    #monitor='val_mse',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        "),\n",
        "tf.keras.callbacks.CSVLogger(\n",
        "    filename=checkpoints_path + '.csv')\n",
        "]\n",
        "\n",
        "model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, steps_per_epoch=steps_per_epoch, \n",
        "          validation_steps=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "112/112 [==============================] - 265s 2s/step - loss: 56.8978 - normal_output_loss: 56.2471 - mask_output_loss: 0.6507 - val_loss: 52.3799 - val_normal_output_loss: 51.8493 - val_mask_output_loss: 0.5307\n",
            "Epoch 2/100\n",
            " 42/112 [==========>...................] - ETA: 1:41 - loss: 58.2195 - normal_output_loss: 57.6309 - mask_output_loss: 0.5886"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e4f59c9fa86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, steps_per_epoch=steps_per_epoch, \n\u001b[0;32m---> 25\u001b[0;31m           validation_steps=40)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d45a71b"
      },
      "source": [
        "print(start, end)\n",
        "print(\"Starting time: \", time.ctime(start))\n",
        "print(\"Ending time: \", time.ctime(end))\n",
        "print(\"Time elapsed: \", datetime.timedelta(seconds=round(end - start)))\n",
        "print('')\n",
        "print(\"Number of parameters: \", model.count_params())\n",
        "print(\"\")\n",
        "model.save(model_path + '/' + model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "816f856c"
      },
      "source": [
        "logging.info(\"training model...\")\n",
        "# TODO properly compute steps for progress bar (low priority)\n",
        "steps_per_epoch = len(train_files) * 10 // batch_size\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoints_path, \n",
        "        save_weights_only=True,\n",
        "        monitor='val_mse',\n",
        "        mode='max',\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83c5e4dc"
      },
      "source": [
        "model.summary()\n",
        "model.count_params()\n",
        "#tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}