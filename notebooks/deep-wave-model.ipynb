{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtins\n",
    "import locale\n",
    "import math\n",
    "import glob\n",
    "import pathlib\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "# numerical stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Conv2DTranspose, Reshape\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Input, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'gs://bathy_sample/processed/20211013/combined_data'\n",
    "all_checkpoints_path = 'gs://bathy_sample/dnn/checkpoints'\n",
    "model_name = 'guus-2d-mlp-cnn-v0.1'\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 20\n",
    "batch_size = 8\n",
    "checkpoints_path = all_checkpoints_path + '/' + model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(eg):\n",
    "    \"\"\"parse an example (or batch of examples, not quite sure...)\"\"\"\n",
    "\n",
    "    # here we re-specify our format\n",
    "    # you can also infer the format from the data using tf.train.Example.FromString\n",
    "    # but that did not work\n",
    "    example = tf.io.parse_example(\n",
    "        eg[tf.newaxis],\n",
    "        {\n",
    "            \"height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"depth\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"bathy\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"hs\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"eta\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"zeta\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"theta\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        },\n",
    "    )\n",
    "    bathy = tf.io.parse_tensor(example[\"bathy\"][0], out_type=\"float32\")\n",
    "    hs = tf.io.parse_tensor(example[\"hs\"][0], out_type=\"float32\")\n",
    "    eta = example[\"eta\"]\n",
    "    zeta = example[\"zeta\"]\n",
    "    theta = example[\"theta\"]\n",
    "    attr = tf.stack([eta, zeta, theta], axis=1)\n",
    "    attr = tf.reshape(attr, shape=[-1])\n",
    "    return (bathy, attr), hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_path):\n",
    "    files = tf.io.gfile.glob(data_path + \"/\" + \"*.tfrecords\")\n",
    "    return files\n",
    "\n",
    "def get_dataset(files):\n",
    "    \"\"\"return a tfrecord dataset with all tfrecord files\"\"\"\n",
    "    dataset =  tf.data.TFRecordDataset(files)\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(1024, activation=\"relu\"))\n",
    "    model.add(Dense(256 * 256, activation=\"relu\"))\n",
    "    model.build((None, 256 * 256))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnn(width, height, depth):\n",
    "    input_shape = (height, width, depth)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\")(inputs)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    ax = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Branch 1\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(ax)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    bx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Branch 2\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(bx)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    cx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Branch 3\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(cx)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), padding=\"same\")(x)\n",
    "\n",
    "    # Branch 2\n",
    "    x = Concatenate()([x, cx])\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), padding=\"same\")(x)\n",
    "\n",
    "    # Branch 1\n",
    "    x = Concatenate()([x, bx])\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), padding=\"same\")(x)\n",
    "\n",
    "    # Main Branch\n",
    "    x = Concatenate()([x, ax])\n",
    "    x = Conv2D(16, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def full_model(cnn_model, mlp_model):\n",
    "\n",
    "    x = cnn_model.output\n",
    "    cx = mlp_model.output\n",
    "\n",
    "    conv_shape = K.int_shape(x)\n",
    "\n",
    "    cx = Reshape((conv_shape[1], conv_shape[2], int(conv_shape[3] / 4)))(cx)\n",
    "\n",
    "    x = Concatenate()([x, cx])\n",
    "\n",
    "    x = Conv2D(16, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2DTranspose(1, (3, 3), padding=\"same\", activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[cnn_model.input, mlp_model.input], outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 26)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_files(data_path)\n",
    "train_files, test_files = train_test_split(files)\n",
    "train_dataset = get_dataset(train_files)\n",
    "test_dataset = get_dataset(test_files)\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "len(train_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_dataset(files[1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = create_cnn(256, 256, 1)\n",
    "mlp_model = create_mlp(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = full_model(cnn_model, mlp_model)\n",
    "opt = Adam(learning_rate=learning_rate, decay=learning_rate / n_epochs)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoints_path)\n",
    "]\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 2/95 [..............................] - ETA: 10:02 - loss: 34.7611"
     ]
    }
   ],
   "source": [
    "logging.info(\"training model...\")\n",
    "steps_per_epoch = len(train_files) * 10 // batch_size\n",
    "\n",
    "model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ModelV5.h5')\n",
    "\n",
    "\n",
    "\n",
    "model = load_model('/content/drive/MyDrive/DeepLearning/ModelV11.h5', compile = True)\n",
    "\n",
    "\n",
    "source = 9\n",
    "\n",
    "Prediction = model.predict([inputImages[318:319], inputAttr[318:319]])[0][:,:,0]\n",
    "Truehs = outputImages[318][:,:,0]\n",
    "#Prediction = model.predict([testImgX[0:10], testAttrX[0:10]])[source][:,:,0]\n",
    "Prediction[Prediction < 0] = np.nan\n",
    "#Truehs = testY[source][:,:,0]\n",
    "Truehs[Truehs < 0] = np.nan\n",
    "#print(testAttrX[6])\n",
    "fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.set_title('colorMap')\n",
    "plt.imshow(Prediction)\n",
    "\n",
    "qx = fig.add_subplot(1,2,2)\n",
    "plt.imshow(Truehs)\n",
    "\n",
    "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "cax.get_xaxis().set_visible(False)\n",
    "cax.get_yaxis().set_visible(False)\n",
    "cax.patch.set_alpha(0)\n",
    "cax.set_frame_on(False)\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
