{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51f7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtins\n",
    "import locale\n",
    "import math\n",
    "import glob\n",
    "import pathlib\n",
    "import functools\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# numerical stuff\n",
    "import pickle5 as pickle\n",
    "import tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67a72ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pickle5' from '/home/guusvh/venvs/main/lib/python3.6/site-packages/pickle5/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# monkey patch the tables library to support pickle5, files were created using Python 3.8\n",
    "tables.atom.pickle = pickle\n",
    "tables.atom.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14066ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Conv2DTranspose, Reshape\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Input, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python import ipu\n",
    "\n",
    "import libpvti as pvti\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd99a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fri_Oct_29_06:10:05_2021_EDT_160706.pvti'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard IPU TensorFlow setup.\n",
    "ipu_config = ipu.config.IPUConfig()\n",
    "ipu_config.auto_select_ipus = 2\n",
    "ipu_config.configure_ipu_system()\n",
    "\n",
    "# Create an execution strategy.\n",
    "strategy = ipu.ipu_strategy.IPUStrategy()\n",
    "\n",
    "# Configure profiling\n",
    "pvti.getCurrentTraceFilename()\n",
    "#!export PVTI_OPTIONS='{\"enable\":\"true\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed43b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = 'gs://bathy_sample/processed/20211013/train_data_mask_no_schematic'\n",
    "data_path_test = 'gs://bathy_sample/processed/20211013/test_data_mask_no_schematic'\n",
    "meta_data_path = '/mnt/poddata/data/bathy-emodnet-a-runs.h5'\n",
    "#all_checkpoints_path = 'gs://bathy_sample/dnn/checkpoints'\n",
    "all_checkpoints_path = '/mnt/poddata/data/checkpoints'\n",
    "model_name = 'guus-2d-mlp-cnn-v0.6'\n",
    "model_path = '/mnt/poddata/data/models'\n",
    "checkpoints_path = all_checkpoints_path + '/' + model_name\n",
    "\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 100\n",
    "batch_size = 1\n",
    "steps_per_execution = 32\n",
    "steps_per_epoch = 896\n",
    "validation_steps = 320\n",
    "gradient_accumulation_steps_per_replica = 8\n",
    "raster_shape = (256, 256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eaa8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(eg):\n",
    "    \"\"\"parse an example (or batch of examples, not quite sure...)\"\"\"\n",
    "\n",
    "    # here we re-specify our format\n",
    "    # you can also infer the format from the data using tf.train.Example.FromString\n",
    "    # but that did not work\n",
    "    example = tf.io.parse_example(\n",
    "        eg[tf.newaxis],\n",
    "        {\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'bathy': tf.io.FixedLenFeature([], tf.string),\n",
    "            'hs': tf.io.FixedLenFeature([], tf.string),\n",
    "            'tm01': tf.io.FixedLenFeature([], tf.string),\n",
    "            'theta0x': tf.io.FixedLenFeature([], tf.string),\n",
    "            'theta0y': tf.io.FixedLenFeature([], tf.string),\n",
    "            'eta': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'zeta': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'theta_wavex': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'theta_wavey': tf.io.FixedLenFeature([], tf.float32),\n",
    "        },\n",
    "    )\n",
    "    bathy = tf.io.parse_tensor(example[\"bathy\"][0], out_type=\"float32\")\n",
    "    bathy = tf.ensure_shape(bathy, raster_shape)    # ensure shape, to be able to train the model\n",
    "    hs = tf.io.parse_tensor(example[\"hs\"][0], out_type=\"float32\")\n",
    "    hs = tf.ensure_shape(hs, raster_shape)\n",
    "    tm01 = tf.io.parse_tensor(example[\"tm01\"][0], out_type=\"float32\")\n",
    "    theta0x = tf.io.parse_tensor(example[\"theta0x\"][0], out_type=\"float32\")\n",
    "    theta0y = tf.io.parse_tensor(example[\"theta0y\"][0], out_type=\"float32\")\n",
    "    eta = example[\"eta\"]\n",
    "    zeta = example[\"zeta\"]\n",
    "    theta_wavex = example[\"theta_wavex\"]\n",
    "    theta_wavey = example[\"theta_wavey\"]\n",
    "    attr = tf.stack([eta, zeta, theta_wavex, theta_wavey], axis=1)\n",
    "    attr = tf.reshape(attr,shape=[-1])\n",
    "    output = (hs, tm01, theta0x, theta0y)\n",
    "    output = hs\n",
    "    return (bathy, attr), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_path):\n",
    "    files = tf.io.gfile.glob(data_path + \"/\" + \"*.tfrecords\")\n",
    "    return files\n",
    "\n",
    "def get_dataset(files):\n",
    "    \"\"\"return a tfrecord dataset with all tfrecord files\"\"\"\n",
    "    dataset =  tf.data.TFRecordDataset(files)\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f5c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(cnn_input_shape, mlp_input_shape):\n",
    "    \n",
    "    mlp_input = Input(mlp_input_shape)\n",
    "    cnn_input = Input(cnn_input_shape)\n",
    "    with ipu.keras.PipelineStage(0):\n",
    "        x = Dense(256, activation='relu')(mlp_input)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        mlp_output = Dense(4, activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    #with ipu.keras.PipelineStage(1):\n",
    "        x = Conv2D(16, (3,3), padding=\"same\")(cnn_input)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "        x = Conv2D(32, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "        x = Conv2D(64, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "        x = Conv2D(128, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "        \n",
    "        x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        cnn_output = BatchNormalization()(x)\n",
    "    \n",
    "    with ipu.keras.PipelineStage(1):\n",
    "        conv_shape = K.int_shape(cnn_output)\n",
    "\n",
    "        x = Flatten()(cnn_output)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(4, activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Concatenate()([x,mlp_output])\n",
    "        x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Reshape((conv_shape[1],conv_shape[2],int(conv_shape[3])))(x)\n",
    "        x = tf.keras.layers.add([x, cnn_output])\n",
    "    \n",
    "    #with ipu.keras.PipelineStage(3):\n",
    "        x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = UpSampling2D(size=(2,2))(x)\n",
    "        x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = UpSampling2D(size=(2,2))(x)\n",
    "        x = Conv2DTranspose(128, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = UpSampling2D(size=(2,2))(x)\n",
    "        x = Conv2DTranspose(64, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Conv2DTranspose(32, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Conv2DTranspose(16, (3,3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = UpSampling2D(size=(2,2))(x)\n",
    "        x = Conv2DTranspose(1, (3,3), padding=\"same\", activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input], outputs = x)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c585d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = get_files(data_path_train)\n",
    "test_files = get_files(data_path_test)\n",
    "\n",
    "train_dataset = get_dataset(train_files)\n",
    "test_dataset = get_dataset(test_files)\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True).repeat().prefetch(16)\n",
    "test_dataset = test_dataset.batch(batch_size, drop_remainder=True).repeat().prefetch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5701e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 7.748603820800781e-05\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_dataset\n",
    "10275*0.7/8\n",
    "10275*0.3/8\n",
    "print('Time elapsed:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a01a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 29 06:11:27 2021\n",
      "Epoch 1/100\n",
      "896/896 [==============================] - 741s 827ms/step - loss: 29.0700 - val_loss: 15.8787\n",
      "Epoch 2/100\n",
      "896/896 [==============================] - 83s 92ms/step - loss: 8.9973 - val_loss: 4.3021\n",
      "Epoch 3/100\n",
      "896/896 [==============================] - 76s 84ms/step - loss: 3.4564 - val_loss: 3.2807\n",
      "Epoch 4/100\n",
      "896/896 [==============================] - 73s 81ms/step - loss: 3.3034 - val_loss: 2.4279\n",
      "Epoch 5/100\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 1.7880 - val_loss: 2.6783\n",
      "Epoch 6/100\n",
      "896/896 [==============================] - 67s 74ms/step - loss: 2.3366 - val_loss: 2.2089\n",
      "Epoch 7/100\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 2.5923 - val_loss: 2.5739\n",
      "Epoch 8/100\n",
      "896/896 [==============================] - 70s 78ms/step - loss: 1.6605 - val_loss: 1.8370\n",
      "Epoch 9/100\n",
      "896/896 [==============================] - 52s 59ms/step - loss: 1.4407 - val_loss: 2.0844\n",
      "Epoch 10/100\n",
      "896/896 [==============================] - 52s 58ms/step - loss: 1.5334 - val_loss: 1.8925\n",
      "Epoch 11/100\n",
      "896/896 [==============================] - 51s 56ms/step - loss: 1.6611 - val_loss: 1.9187\n",
      "Epoch 12/100\n",
      "896/896 [==============================] - 51s 56ms/step - loss: 2.1399 - val_loss: 1.8498\n",
      "Epoch 13/100\n",
      "896/896 [==============================] - 53s 60ms/step - loss: 1.3571 - val_loss: 1.8584\n",
      "Epoch 14/100\n",
      "896/896 [==============================] - 47s 52ms/step - loss: 1.5923 - val_loss: 2.1027\n",
      "Epoch 15/100\n",
      "896/896 [==============================] - 52s 58ms/step - loss: 1.8718 - val_loss: 2.3809\n",
      "Epoch 16/100\n",
      "896/896 [==============================] - 53s 60ms/step - loss: 1.2726 - val_loss: 2.1613\n",
      "Epoch 17/100\n",
      "896/896 [==============================] - 85s 95ms/step - loss: 1.0397 - val_loss: 2.3809\n",
      "Epoch 18/100\n",
      "896/896 [==============================] - 79s 88ms/step - loss: 1.0907 - val_loss: 1.3953\n",
      "Epoch 19/100\n",
      "896/896 [==============================] - 84s 94ms/step - loss: 0.7674 - val_loss: 1.0746\n",
      "Epoch 20/100\n",
      "896/896 [==============================] - 82s 91ms/step - loss: 1.3782 - val_loss: 0.8931\n",
      "Epoch 21/100\n",
      "896/896 [==============================] - 75s 84ms/step - loss: 0.6461 - val_loss: 0.8078\n",
      "Epoch 22/100\n",
      "896/896 [==============================] - 71s 79ms/step - loss: 0.6329 - val_loss: 0.9187\n",
      "Epoch 23/100\n",
      "896/896 [==============================] - 63s 71ms/step - loss: 1.0221 - val_loss: 1.9635\n",
      "Epoch 24/100\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 0.8989 - val_loss: 0.8904\n",
      "Epoch 25/100\n",
      "896/896 [==============================] - 38s 42ms/step - loss: 0.3571 - val_loss: 0.7409\n",
      "Epoch 26/100\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.6009 - val_loss: 0.8713\n",
      "Epoch 27/100\n",
      "896/896 [==============================] - 35s 39ms/step - loss: 0.3821 - val_loss: 1.2643\n",
      "Epoch 28/100\n",
      "896/896 [==============================] - 34s 38ms/step - loss: 0.6205 - val_loss: 0.6137\n",
      "Epoch 29/100\n",
      "896/896 [==============================] - 37s 41ms/step - loss: 0.4941 - val_loss: 0.5972\n",
      "Epoch 30/100\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.4801 - val_loss: 0.5639\n",
      "Epoch 31/100\n",
      "896/896 [==============================] - 50s 56ms/step - loss: 0.7302 - val_loss: 0.6804\n",
      "Epoch 32/100\n",
      "896/896 [==============================] - 45s 50ms/step - loss: 0.4582 - val_loss: 0.5018\n",
      "Epoch 33/100\n",
      "896/896 [==============================] - 55s 61ms/step - loss: 0.2893 - val_loss: 1.2644\n",
      "Epoch 34/100\n",
      "896/896 [==============================] - 55s 61ms/step - loss: 0.6857 - val_loss: 0.6785\n",
      "Epoch 35/100\n",
      "896/896 [==============================] - 52s 58ms/step - loss: 0.3027 - val_loss: 0.5637\n",
      "Epoch 36/100\n",
      "896/896 [==============================] - 55s 62ms/step - loss: 0.4671 - val_loss: 0.5039\n",
      "Epoch 37/100\n",
      "896/896 [==============================] - 59s 66ms/step - loss: 0.3759 - val_loss: 0.5820\n",
      "Epoch 38/100\n",
      "896/896 [==============================] - 61s 68ms/step - loss: 0.4852 - val_loss: 0.5077\n",
      "Epoch 39/100\n",
      "896/896 [==============================] - 62s 69ms/step - loss: 0.5204 - val_loss: 0.6114\n",
      "Epoch 40/100\n",
      "896/896 [==============================] - 53s 60ms/step - loss: 0.3185 - val_loss: 0.4037\n",
      "Epoch 41/100\n",
      "896/896 [==============================] - 51s 57ms/step - loss: 0.2388 - val_loss: 1.2107\n",
      "Epoch 42/100\n",
      "896/896 [==============================] - 50s 56ms/step - loss: 0.7391 - val_loss: 0.5694\n",
      "Epoch 43/100\n",
      "896/896 [==============================] - 51s 57ms/step - loss: 0.2549 - val_loss: 0.3999\n",
      "Epoch 44/100\n",
      "896/896 [==============================] - 52s 58ms/step - loss: 0.3369 - val_loss: 0.3495\n",
      "Epoch 45/100\n",
      "896/896 [==============================] - 52s 58ms/step - loss: 0.2855 - val_loss: 0.5666\n",
      "Epoch 46/100\n",
      "896/896 [==============================] - 52s 58ms/step - loss: 0.3387 - val_loss: 0.3796\n",
      "Epoch 47/100\n",
      "896/896 [==============================] - 48s 54ms/step - loss: 0.3547 - val_loss: 0.7924\n",
      "Epoch 48/100\n",
      "896/896 [==============================] - 47s 53ms/step - loss: 0.3590 - val_loss: 0.4265\n",
      "Epoch 49/100\n",
      "896/896 [==============================] - 43s 48ms/step - loss: 0.1973 - val_loss: 1.6353\n",
      "Epoch 50/100\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.6495 - val_loss: 0.4200\n",
      "Epoch 51/100\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.2660 - val_loss: 0.3718\n",
      "Epoch 52/100\n",
      "896/896 [==============================] - 42s 47ms/step - loss: 0.2677 - val_loss: 0.3359\n",
      "Epoch 53/100\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.2190 - val_loss: 0.3360\n",
      "Epoch 54/100\n",
      "896/896 [==============================] - 49s 55ms/step - loss: 0.3875 - val_loss: 0.3179\n",
      "Epoch 55/100\n",
      "896/896 [==============================] - 48s 54ms/step - loss: 0.3010 - val_loss: 0.9067\n",
      "Epoch 56/100\n",
      "896/896 [==============================] - 44s 49ms/step - loss: 0.2697 - val_loss: 0.8401\n",
      "Epoch 57/100\n",
      "896/896 [==============================] - 46s 52ms/step - loss: 0.1812 - val_loss: 2.5524\n",
      "Epoch 58/100\n",
      "896/896 [==============================] - 44s 49ms/step - loss: 0.5344 - val_loss: 0.4227\n",
      "Epoch 59/100\n",
      "896/896 [==============================] - 43s 49ms/step - loss: 0.2462 - val_loss: 0.3535\n",
      "Epoch 60/100\n",
      "896/896 [==============================] - 46s 51ms/step - loss: 0.2966 - val_loss: 0.4376\n",
      "Epoch 61/100\n",
      "896/896 [==============================] - 45s 50ms/step - loss: 0.2284 - val_loss: 0.3104\n",
      "Epoch 62/100\n",
      "896/896 [==============================] - 46s 51ms/step - loss: 0.3054 - val_loss: 0.3332\n",
      "Epoch 63/100\n",
      "896/896 [==============================] - 46s 51ms/step - loss: 0.2545 - val_loss: 0.5163\n",
      "Epoch 64/100\n",
      "896/896 [==============================] - 46s 51ms/step - loss: 0.2396 - val_loss: 0.7099\n",
      "Epoch 65/100\n",
      "896/896 [==============================] - 45s 50ms/step - loss: 0.1732 - val_loss: 2.2707\n",
      "Epoch 66/100\n",
      "896/896 [==============================] - 45s 51ms/step - loss: 0.5699 - val_loss: 0.3359\n",
      "Epoch 67/100\n",
      "896/896 [==============================] - 44s 49ms/step - loss: 0.2228 - val_loss: 0.2916\n",
      "Epoch 68/100\n",
      "896/896 [==============================] - 43s 48ms/step - loss: 0.2643 - val_loss: 0.3750\n",
      "Epoch 69/100\n",
      "896/896 [==============================] - 43s 48ms/step - loss: 0.2388 - val_loss: 0.4238\n",
      "Epoch 70/100\n",
      "896/896 [==============================] - 45s 50ms/step - loss: 0.2325 - val_loss: 0.3365\n",
      "Epoch 71/100\n",
      "896/896 [==============================] - 45s 50ms/step - loss: 0.2651 - val_loss: 0.3648\n",
      "Epoch 72/100\n",
      "896/896 [==============================] - 41s 46ms/step - loss: 0.1936 - val_loss: 1.0544\n",
      "Epoch 73/100\n",
      "896/896 [==============================] - 39s 44ms/step - loss: 0.1579 - val_loss: 1.4602\n",
      "Epoch 74/100\n",
      "896/896 [==============================] - 43s 48ms/step - loss: 0.4598 - val_loss: 0.3551\n",
      "Epoch 75/100\n",
      "896/896 [==============================] - 39s 43ms/step - loss: 0.2062 - val_loss: 0.3694\n",
      "Epoch 76/100\n",
      "896/896 [==============================] - 41s 46ms/step - loss: 0.2592 - val_loss: 0.5628\n",
      "Epoch 77/100\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.2099 - val_loss: 0.2436\n",
      "Epoch 78/100\n",
      "896/896 [==============================] - 41s 46ms/step - loss: 0.2203 - val_loss: 0.4234\n",
      "Epoch 79/100\n",
      "896/896 [==============================] - 42s 46ms/step - loss: 0.2910 - val_loss: 0.4324\n",
      "Epoch 80/100\n",
      "896/896 [==============================] - 39s 44ms/step - loss: 0.2350 - val_loss: 0.5254\n",
      "Epoch 81/100\n",
      "896/896 [==============================] - 38s 43ms/step - loss: 0.1453 - val_loss: 2.4869\n",
      "Epoch 82/100\n",
      "896/896 [==============================] - 39s 44ms/step - loss: 0.5202 - val_loss: 0.3489\n",
      "Epoch 83/100\n",
      "896/896 [==============================] - 39s 44ms/step - loss: 0.1815 - val_loss: 0.3186\n",
      "Epoch 84/100\n",
      "896/896 [==============================] - 42s 46ms/step - loss: 0.1848 - val_loss: 0.2878\n",
      "Epoch 85/100\n",
      "896/896 [==============================] - 41s 45ms/step - loss: 0.1950 - val_loss: 0.2680\n",
      "Epoch 86/100\n",
      "896/896 [==============================] - 41s 46ms/step - loss: 0.2158 - val_loss: 0.4033\n",
      "Epoch 87/100\n",
      "896/896 [==============================] - 38s 42ms/step - loss: 0.2336 - val_loss: 0.3782\n",
      "Epoch 88/100\n",
      "896/896 [==============================] - 38s 42ms/step - loss: 0.2312 - val_loss: 0.3918\n",
      "Epoch 89/100\n",
      "896/896 [==============================] - 40s 45ms/step - loss: 0.1305 - val_loss: 1.1209\n",
      "Epoch 90/100\n",
      "896/896 [==============================] - 41s 45ms/step - loss: 0.4169 - val_loss: 0.3171\n",
      "Epoch 91/100\n",
      "896/896 [==============================] - 38s 43ms/step - loss: 0.1499 - val_loss: 0.2602\n",
      "Epoch 92/100\n",
      "896/896 [==============================] - 77s 86ms/step - loss: 0.1544 - val_loss: 0.4440\n",
      "Epoch 93/100\n",
      "896/896 [==============================] - 74s 83ms/step - loss: 0.1777 - val_loss: 0.3070\n",
      "Epoch 94/100\n",
      "896/896 [==============================] - 72s 80ms/step - loss: 0.2077 - val_loss: 0.3248\n",
      "Epoch 95/100\n",
      "896/896 [==============================] - 68s 76ms/step - loss: 0.2017 - val_loss: 0.2862\n",
      "Epoch 96/100\n",
      "896/896 [==============================] - 65s 73ms/step - loss: 0.2438 - val_loss: 0.2615\n",
      "Epoch 97/100\n",
      "896/896 [==============================] - 70s 78ms/step - loss: 0.1126 - val_loss: 0.5208\n",
      "Epoch 98/100\n",
      "896/896 [==============================] - 69s 77ms/step - loss: 0.2748 - val_loss: 0.2267\n",
      "Epoch 99/100\n",
      "896/896 [==============================] - 67s 74ms/step - loss: 0.1429 - val_loss: 0.2665\n",
      "Epoch 100/100\n",
      "896/896 [==============================] - 50s 56ms/step - loss: 0.1814 - val_loss: 0.2819\n",
      "Fri Oct 29 07:49:15 2021\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(time.ctime(start))\n",
    "with strategy.scope():\n",
    "    model = full_model((256, 256, 1), 4)\n",
    "    opt = Adam(learning_rate=learning_rate, decay=learning_rate / n_epochs)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt, steps_per_execution=steps_per_execution)\n",
    "    \n",
    "    callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoints_path, \n",
    "        save_weights_only=True,\n",
    "        #monitor='val_mse',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        filename=checkpoints_path + '.csv')\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.set_gradient_accumulation_options(\n",
    "        gradient_accumulation_steps_per_replica=gradient_accumulation_steps_per_replica)\n",
    "    model.set_pipelining_options(\n",
    "        gradient_accumulation_steps_per_replica=gradient_accumulation_steps_per_replica)\n",
    "    model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, steps_per_epoch=steps_per_epoch,\n",
    "             validation_steps=validation_steps, callbacks=callbacks)\n",
    "    #model.save(model_path + '/' + model_name + '.h5')\n",
    "\n",
    "end = time.time()\n",
    "print(time.ctime(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d45a71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635502287.8490686 1635508155.6447768\n",
      "Starting time:  Fri Oct 29 06:11:27 2021\n",
      "Ending time:  Fri Oct 29 07:49:15 2021\n",
      "Time elapsed:  1:37:48\n",
      "\n",
      "Number of parameters:  8244633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(start, end)\n",
    "print(\"Starting time: \", time.ctime(start))\n",
    "print(\"Ending time: \", time.ctime(end))\n",
    "print(\"Time elapsed: \", datetime.timedelta(seconds=round(end - start)))\n",
    "print('')\n",
    "print(\"Number of parameters: \", model.count_params())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"training model...\")\n",
    "# TODO properly compute steps for progress bar (low priority)\n",
    "steps_per_epoch = len(train_files) * 10 // batch_size\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoints_path, \n",
    "        save_weights_only=True,\n",
    "        monitor='val_mse',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83c5e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 16) 160         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 16) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 16) 64          activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 256)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 256)  590080      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 256)  1024        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 65536)        0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           4194368     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1280        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           256         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16)           64          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            68          batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           528         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4)            16          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            68          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8)            0           batch_normalization_8[0][0]      \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 65536)        589824      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 65536)        262144      dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 16, 16, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 16, 256)  0           reshape[0][0]                    \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  590080      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 256)  590080      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 256)  590080      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 256)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 128)  295040      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 128)  0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 64) 73792       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 128, 64) 0           conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128, 128, 64) 256         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 128, 128, 32) 18464       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 128, 32) 0           conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 32) 128         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 128, 128, 16) 4624        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 16) 0           conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 256, 256, 1)  145         up_sampling2d_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 8,244,633\n",
      "Trainable params: 8,109,873\n",
      "Non-trainable params: 134,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8244633"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.count_params()\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e59b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\zeta$</th>\n",
       "      <th>$\\theta_{wave}$</th>\n",
       "      <th>$\\eta$</th>\n",
       "      <th>$bathy_i$</th>\n",
       "      <th>bathy_file</th>\n",
       "      <th>bathy</th>\n",
       "      <th>uuid</th>\n",
       "      <th>run_id</th>\n",
       "      <th>bathy_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-1.2872086, -1.2872086, -1.2872086, -1.48720...</td>\n",
       "      <td>88c98d1f-6307-43b5-a8bd-0cc6b23e105e</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>462</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-13.052446, -12.792448, -12.792448, -12.5124...</td>\n",
       "      <td>b38a89ab-f021-42dc-808e-e116dd924a46</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-1.2872086, -1.2872086, -1.2872086, -1.48720...</td>\n",
       "      <td>318908b3-1ed1-4117-8226-6eaeb2846a24</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>462</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-13.052446, -12.792448, -12.792448, -12.5124...</td>\n",
       "      <td>dee4ae77-a3af-4e73-9f16-54b7ac589158</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.283185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-1.2872086, -1.2872086, -1.2872086, -1.48720...</td>\n",
       "      <td>e7bb6f7b-69db-4515-9dcf-6dc009d4d775</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>13.282544</td>\n",
       "      <td>3.631269</td>\n",
       "      <td>8.666409</td>\n",
       "      <td>100</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-21.735966, -21.735966, -21.635967, -21.6359...</td>\n",
       "      <td>65382ebd-1b80-4d75-9287-8bb7f2a41db0</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>13.581516</td>\n",
       "      <td>2.358778</td>\n",
       "      <td>29.419263</td>\n",
       "      <td>402</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[6.6918297, 6.6918297, 6.6918297, 6.6918297, ...</td>\n",
       "      <td>0402bcb5-7309-4662-8a18-93f1dccde9b4</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>8.416452</td>\n",
       "      <td>5.184157</td>\n",
       "      <td>14.352605</td>\n",
       "      <td>151</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[-1.3258033, -1.3258033, -1.3258033, -1.32580...</td>\n",
       "      <td>065553d1-f60d-4d7e-997e-0b07747682be</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1.511433</td>\n",
       "      <td>5.741217</td>\n",
       "      <td>39.150158</td>\n",
       "      <td>367</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[20.617117, 20.617117, 20.017117, 20.017117, ...</td>\n",
       "      <td>06577167-1089-49f4-aea7-2f736f9398ca</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>11.297443</td>\n",
       "      <td>6.277600</td>\n",
       "      <td>5.148588</td>\n",
       "      <td>452</td>\n",
       "      <td>/p/11207539-001-undeepwaves/emodnet/bathymetry...</td>\n",
       "      <td>[[18.348408, 18.348408, 17.188408, 15.788408, ...</td>\n",
       "      <td>69686c04-6342-4e3b-9807-80390d75e21e</td>\n",
       "      <td>a</td>\n",
       "      <td>emodnet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        $\\zeta$  $\\theta_{wave}$     $\\eta$  $bathy_i$  \\\n",
       "0      1.000000         0.000000   1.000000          0   \n",
       "1      1.000000         0.000000   1.000000        462   \n",
       "2      1.000000         0.000000  50.000000          0   \n",
       "3      1.000000         0.000000  50.000000        462   \n",
       "4      1.000000         6.283185   1.000000          0   \n",
       "...         ...              ...        ...        ...   \n",
       "1011  13.282544         3.631269   8.666409        100   \n",
       "1012  13.581516         2.358778  29.419263        402   \n",
       "1013   8.416452         5.184157  14.352605        151   \n",
       "1014   1.511433         5.741217  39.150158        367   \n",
       "1015  11.297443         6.277600   5.148588        452   \n",
       "\n",
       "                                             bathy_file  \\\n",
       "0     /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "1     /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "2     /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "3     /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "4     /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "...                                                 ...   \n",
       "1011  /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "1012  /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "1013  /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "1014  /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "1015  /p/11207539-001-undeepwaves/emodnet/bathymetry...   \n",
       "\n",
       "                                                  bathy  \\\n",
       "0     [[-1.2872086, -1.2872086, -1.2872086, -1.48720...   \n",
       "1     [[-13.052446, -12.792448, -12.792448, -12.5124...   \n",
       "2     [[-1.2872086, -1.2872086, -1.2872086, -1.48720...   \n",
       "3     [[-13.052446, -12.792448, -12.792448, -12.5124...   \n",
       "4     [[-1.2872086, -1.2872086, -1.2872086, -1.48720...   \n",
       "...                                                 ...   \n",
       "1011  [[-21.735966, -21.735966, -21.635967, -21.6359...   \n",
       "1012  [[6.6918297, 6.6918297, 6.6918297, 6.6918297, ...   \n",
       "1013  [[-1.3258033, -1.3258033, -1.3258033, -1.32580...   \n",
       "1014  [[20.617117, 20.617117, 20.017117, 20.017117, ...   \n",
       "1015  [[18.348408, 18.348408, 17.188408, 15.788408, ...   \n",
       "\n",
       "                                      uuid run_id bathy_source  \n",
       "0     88c98d1f-6307-43b5-a8bd-0cc6b23e105e      a      emodnet  \n",
       "1     b38a89ab-f021-42dc-808e-e116dd924a46      a      emodnet  \n",
       "2     318908b3-1ed1-4117-8226-6eaeb2846a24      a      emodnet  \n",
       "3     dee4ae77-a3af-4e73-9f16-54b7ac589158      a      emodnet  \n",
       "4     e7bb6f7b-69db-4515-9dcf-6dc009d4d775      a      emodnet  \n",
       "...                                    ...    ...          ...  \n",
       "1011  65382ebd-1b80-4d75-9287-8bb7f2a41db0      a      emodnet  \n",
       "1012  0402bcb5-7309-4662-8a18-93f1dccde9b4      a      emodnet  \n",
       "1013  065553d1-f60d-4d7e-997e-0b07747682be      a      emodnet  \n",
       "1014  06577167-1089-49f4-aea7-2f736f9398ca      a      emodnet  \n",
       "1015  69686c04-6342-4e3b-9807-80390d75e21e      a      emodnet  \n",
       "\n",
       "[1016 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf(meta_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c60d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
