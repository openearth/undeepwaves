{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_wave_multi_output.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsm6jCoXJSum",
        "outputId": "cb98c0cb-6946-49a3-a815-5d8d48d0b1ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a51f7778"
      },
      "source": [
        "# builtins\n",
        "import locale\n",
        "import math\n",
        "import glob\n",
        "import pathlib\n",
        "import functools\n",
        "import logging\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# numerical stuff\n",
        "#import pickle5 as pickle\n",
        "import tables\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Conv2DTranspose, Reshape, Lambda\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Input, UpSampling2D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "#from tensorflow.python import ipu\n",
        "\n",
        "#import libpvti as pvti\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmKbYE3o1TQL",
        "outputId": "c350ba3f-e484-4961-bba5-d52dfbec680b"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.120.199.178:8470']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.120.199.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.120.199.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ed43b62"
      },
      "source": [
        "data_path_train = 'gs://bathy_sample/processed/20211013/train_data_mask_no_schematic'\n",
        "data_path_test = 'gs://bathy_sample/processed/20211013/test_data_mask_no_schematic'\n",
        "#meta_data_path = '/mnt/poddata/data/bathy-emodnet-a-runs.h5'\n",
        "#all_checkpoints_path = 'gs://bathy_sample/dnn/checkpoints'\n",
        "all_checkpoints_path = '/content/drive/MyDrive/DeepLearning/Benchmark/checkpoints'\n",
        "model_name = 'guus-2d-mlp-cnn-v2.0'\n",
        "model_path = '/content/drive/MyDrive/DeepLearning/Benchmark/mask_models'\n",
        "checkpoints_path = all_checkpoints_path + '/' + model_name\n",
        "\n",
        "learning_rate = 1e-4\n",
        "n_epochs = 100\n",
        "batch_size = 8 * tpu_strategy.num_replicas_in_sync\n",
        "steps_per_execution = 1\n",
        "steps_per_epoch = 112\n",
        "validation_steps = 320\n",
        "gradient_accumulation_steps_per_replica = 8\n",
        "raster_shape = (256, 256, 1)\n",
        "input_attributes = np.load('/content/drive/MyDrive/DeepLearning/input_attributes.npy', allow_pickle=True).item()\n",
        "(eta_mean, eta_std) = input_attributes['eta'].values()\n",
        "(zeta_mean, zeta_std) = input_attributes['zeta'].values()\n",
        "(bathy_mean, bathy_std) = input_attributes['bathy'].values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eaa8178"
      },
      "source": [
        "def tf_parse(eg):\n",
        "    \"\"\"parse an example (or batch of examples, not quite sure...)\"\"\"\n",
        "\n",
        "    # here we re-specify our format\n",
        "    # you can also infer the format from the data using tf.train.Example.FromString\n",
        "    # but that did not work\n",
        "    example = tf.io.parse_example(\n",
        "        eg[tf.newaxis],\n",
        "        {\n",
        "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'bathy': tf.io.FixedLenFeature([], tf.string),\n",
        "            'hs': tf.io.FixedLenFeature([], tf.string),\n",
        "            'tm01': tf.io.FixedLenFeature([], tf.string),\n",
        "            'theta0x': tf.io.FixedLenFeature([], tf.string),\n",
        "            'theta0y': tf.io.FixedLenFeature([], tf.string),\n",
        "            'eta': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'zeta': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'theta_wavex': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'theta_wavey': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'mask': tf.io.FixedLenFeature([], tf.string),\n",
        "        },\n",
        "    )\n",
        "    bathy = tf.io.parse_tensor(example[\"bathy\"][0], out_type=\"float32\")\n",
        "    bathy = tf.ensure_shape(bathy, raster_shape)    # ensure shape, to be able to train the model\n",
        "    hs = tf.io.parse_tensor(example[\"hs\"][0], out_type=\"float32\")\n",
        "    hs = tf.ensure_shape(hs, raster_shape)\n",
        "    tm01 = tf.io.parse_tensor(example[\"tm01\"][0], out_type=\"float32\")\n",
        "    tm01 = tf.ensure_shape(tm01, raster_shape)\n",
        "    theta0x = tf.io.parse_tensor(example[\"theta0x\"][0], out_type=\"float32\")\n",
        "    theta0x = tf.ensure_shape(theta0x, raster_shape)\n",
        "    theta0y = tf.io.parse_tensor(example[\"theta0y\"][0], out_type=\"float32\")\n",
        "    theta0y = tf.ensure_shape(theta0y, raster_shape)\n",
        "    eta = example[\"eta\"]\n",
        "    zeta = example[\"zeta\"]\n",
        "    theta_wavex = example[\"theta_wavex\"]\n",
        "    theta_wavey = example[\"theta_wavey\"]\n",
        "    # Angles were in degrees instead of radians when dataset created, so adjust theta_wavex and theta_wavey\n",
        "    theta_wave = (tf.math.atan2(theta_wavey, theta_wavex) + 2*np.pi) % (2*np.pi) * np.pi / 180.\n",
        "    theta_wavex = tf.math.cos(theta_wave)\n",
        "    theta_wavey = tf.math.sin(theta_wave)\n",
        "    # Output mask\n",
        "    mask = tf.io.parse_tensor(example[\"mask\"][0], out_type=\"bool\")\n",
        "    mask = tf.cast(mask, dtype=\"int16\")\n",
        "    mask = tf.ensure_shape(mask, raster_shape)\n",
        "    # Create mask for input where waterlevel is less than bathymetry\n",
        "    bathy_mask = tf.math.greater(bathy * bathy_std + bathy_mean, eta * eta_std + eta_mean)\n",
        "    bathy_mask = tf.cast(bathy_mask, dtype=\"float32\")\n",
        "    img_input = tf.concat([bathy,bathy_mask],-1)\n",
        "    attr = tf.stack([eta, zeta, theta_wavex, theta_wavey], axis=1)\n",
        "    attr = tf.reshape(attr,shape=[-1])\n",
        "    output = tf.concat([hs, tm01, theta0x, theta0y, bathy_mask], axis=-1)\n",
        "    return (img_input, attr), (output, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85f602af"
      },
      "source": [
        "def get_files(data_path):\n",
        "    files = tf.io.gfile.glob(data_path + \"/\" + \"*.tfrecords\")\n",
        "    return files\n",
        "\n",
        "def get_dataset(files):\n",
        "    \"\"\"return a tfrecord dataset with all tfrecord files\"\"\"\n",
        "    dataset =  tf.data.TFRecordDataset(files)\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9OjbHWFSu4s"
      },
      "source": [
        "def AttnBlock2D(x, g, inter_channel):\n",
        "\n",
        "    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1])(x)\n",
        "\n",
        "    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1])(g)\n",
        "\n",
        "    f = Activation('relu')(tf.keras.layers.add([theta_x, phi_g]))\n",
        "\n",
        "    psi_f = Conv2D(1, [1, 1], strides=[1, 1])(f)\n",
        "\n",
        "    rate = Activation('sigmoid')(psi_f)\n",
        "\n",
        "    att_x = tf.keras.layers.multiply([x, rate])\n",
        "\n",
        "    return att_x\n",
        "\n",
        "\n",
        "def attention_up_and_concate(down_layer, layer):\n",
        "    \n",
        "    in_channel = down_layer.get_shape().as_list()[3]\n",
        "\n",
        "    up = UpSampling2D(size=(2, 2))(down_layer)\n",
        "    layer = AttnBlock2D(x=layer, g=up, inter_channel=in_channel // 4)\n",
        "    \n",
        "    my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
        "    \n",
        "    concate = my_concat([up, layer])\n",
        "    return concate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47f5c590"
      },
      "source": [
        "def full_model(cnn_input_shape, mlp_input_shape):\n",
        "    \n",
        "    mlp_input = Input(mlp_input_shape)\n",
        "    cnn_input = Input(cnn_input_shape)\n",
        "    x = Dense(256, activation='relu')(mlp_input)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    mlp_output = Dense(4, activation='relu')(x)\n",
        "\n",
        "    x = Conv2D(16, (3,3), padding=\"same\")(cnn_input)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(32, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Downsampling with stride=2\n",
        "    x = Conv2D(64, (3,3), padding=\"same\", strides=(2,2))(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(128, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    cnn_output = BatchNormalization()(x)\n",
        "\n",
        "    conv_shape = K.int_shape(cnn_output)\n",
        "\n",
        "    # Reshape output from MLP to CNN shape to concatenate\n",
        "    x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation=\"relu\")(mlp_output)\n",
        "    x = Reshape((conv_shape[1],conv_shape[2],int(conv_shape[3])))(x)\n",
        "    \n",
        "    last_x = Concatenate()([x,cnn_output])\n",
        "\n",
        "    #################################### Normal output\n",
        "\n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(last_x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Upsampling with stride=2    \n",
        "    x = Conv2DTranspose(128, (3,3), padding=\"same\", strides=(2,2))(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(32, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(16, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "  \n",
        "    normal_output = Conv2DTranspose(4, (3,3), padding=\"same\", activation=\"linear\", name=\"normal_output\")(x)\n",
        "  \n",
        "    ############################### Mask output\n",
        "\n",
        "    x = Conv2D(256, (3,3), padding=\"same\")(last_x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(256, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    # Upsampling with stride=2\n",
        "    x = Conv2DTranspose(128, (3,3), padding=\"same\", strides=(2,2))(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(32, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2DTranspose(16, (3,3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    mask_output = Conv2DTranspose(1, (3,3), padding=\"same\", activation=\"sigmoid\", name=\"mask_output\")(x)\n",
        "\n",
        "    model = Model(inputs=[cnn_input, mlp_input], outputs = [normal_output, mask_output])\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c585d94"
      },
      "source": [
        "train_files = get_files(data_path_train)\n",
        "test_files = get_files(data_path_test)\n",
        "\n",
        "train_dataset = get_dataset(train_files)\n",
        "test_dataset = get_dataset(test_files)\n",
        "\n",
        "#Filter out the bathymetries with heights greater than 100m\n",
        "def filter_fn(x, y):\n",
        "    return tf.less_equal(tf.reduce_max(x[0][...,0]), (100-bathy_mean)/bathy_std)\n",
        "\n",
        "train_dataset = train_dataset.filter(filter_fn)\n",
        "test_dataset = test_dataset.filter(filter_fn)\n",
        "\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uGDag9WqCPX"
      },
      "source": [
        "def masked_mse_loss(y_true, y_pred):\n",
        "    # Get input mask from the output data\n",
        "    mask = y_true[...,-1]\n",
        "    mask = mask[...,tf.newaxis]\n",
        "    # Get the true output data which we want to train\n",
        "    y_true = y_true[...,:-1]\n",
        "    y_pred_temp = tf.math.multiply(y_pred, 1 - mask)\n",
        "    y_true_temp = tf.math.multiply(y_true, 1 - mask)\n",
        "    diff = tf.math.squared_difference(y_true_temp, y_pred_temp)\n",
        "    loss = K.mean(diff, axis=-1)\n",
        "    loss = tf.keras.losses.mean_squared_error(y_true = y_true_temp, y_pred = y_pred_temp)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a01a94a",
        "outputId": "be1eccad-1ccb-4864-cb42-0510e9a2a21e"
      },
      "source": [
        "start = time.time()\n",
        "print(time.ctime(start))\n",
        "with tpu_strategy.scope():\n",
        "    model = full_model((256, 256, 2), 4)\n",
        "    opt = Adam(learning_rate=learning_rate, decay=learning_rate / n_epochs)\n",
        "\n",
        "    losses = {\n",
        "\t  \"normal_output\": \"mean_squared_error\",\n",
        "  \t\"mask_output\": \"binary_crossentropy\",\n",
        "    }\n",
        "    lossWeights = {\"normal_output\": 1.0, \"mask_output\": 1.0}\n",
        "\n",
        "    model.compile(loss=losses, optimizer=opt, steps_per_execution=steps_per_execution)\n",
        "    \n",
        "    callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoints_path, \n",
        "        save_weights_only=True,\n",
        "        #monitor='val_mse',\n",
        "        mode='max',\n",
        "        save_best_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.CSVLogger(\n",
        "        filename=checkpoints_path + '.csv')\n",
        "    ]\n",
        "    \n",
        "    model.fit(x=train_dataset, validation_data=test_dataset, epochs=n_epochs, callbacks=callbacks)#, steps_per_epoch=steps_per_epoch)\n",
        "    model.save(model_path + '/' + model_name + '.h5')\n",
        "\n",
        "end = time.time()\n",
        "print(time.ctime(end))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 27 10:57:46 2021\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 556s 6s/step - loss: 51.2340 - normal_output_loss: 50.6931 - mask_output_loss: 0.5408 - val_loss: 49.1935 - val_normal_output_loss: 48.6412 - val_mask_output_loss: 0.5524\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 61s 656ms/step - loss: 45.3487 - normal_output_loss: 44.9305 - mask_output_loss: 0.4182 - val_loss: 38.4699 - val_normal_output_loss: 38.0854 - val_mask_output_loss: 0.3845\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 61s 656ms/step - loss: 40.5863 - normal_output_loss: 40.2472 - mask_output_loss: 0.3391 - val_loss: 27.0192 - val_normal_output_loss: 26.6940 - val_mask_output_loss: 0.3252\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 61s 656ms/step - loss: 36.5945 - normal_output_loss: 36.3230 - mask_output_loss: 0.2716 - val_loss: 17.5667 - val_normal_output_loss: 17.3700 - val_mask_output_loss: 0.1967\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 61s 657ms/step - loss: 32.9258 - normal_output_loss: 32.7097 - mask_output_loss: 0.2162 - val_loss: 11.1948 - val_normal_output_loss: 11.0955 - val_mask_output_loss: 0.0993\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 61s 656ms/step - loss: 29.2503 - normal_output_loss: 29.0758 - mask_output_loss: 0.1745 - val_loss: 8.5770 - val_normal_output_loss: 8.5104 - val_mask_output_loss: 0.0666\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 60s 653ms/step - loss: 25.9184 - normal_output_loss: 25.7771 - mask_output_loss: 0.1414 - val_loss: 6.2288 - val_normal_output_loss: 6.1806 - val_mask_output_loss: 0.0482\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 61s 659ms/step - loss: 22.7934 - normal_output_loss: 22.6776 - mask_output_loss: 0.1158 - val_loss: 7.1090 - val_normal_output_loss: 7.0751 - val_mask_output_loss: 0.0339\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 20.0173 - normal_output_loss: 19.9207 - mask_output_loss: 0.0966 - val_loss: 23.6687 - val_normal_output_loss: 23.6446 - val_mask_output_loss: 0.0241\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 17.6509 - normal_output_loss: 17.5708 - mask_output_loss: 0.0801 - val_loss: 7.8195 - val_normal_output_loss: 7.7996 - val_mask_output_loss: 0.0199\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 61s 662ms/step - loss: 15.3865 - normal_output_loss: 15.3189 - mask_output_loss: 0.0676 - val_loss: 3.1254 - val_normal_output_loss: 3.1081 - val_mask_output_loss: 0.0173\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 13.5290 - normal_output_loss: 13.4717 - mask_output_loss: 0.0573 - val_loss: 38.3050 - val_normal_output_loss: 38.2917 - val_mask_output_loss: 0.0133\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 11.8095 - normal_output_loss: 11.7602 - mask_output_loss: 0.0493 - val_loss: 4.7811 - val_normal_output_loss: 4.7701 - val_mask_output_loss: 0.0110\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 10.3070 - normal_output_loss: 10.2645 - mask_output_loss: 0.0424 - val_loss: 8.9094 - val_normal_output_loss: 8.8993 - val_mask_output_loss: 0.0101\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 9.0487 - normal_output_loss: 9.0119 - mask_output_loss: 0.0368 - val_loss: 13.7710 - val_normal_output_loss: 13.7622 - val_mask_output_loss: 0.0088\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 61s 659ms/step - loss: 7.9284 - normal_output_loss: 7.8962 - mask_output_loss: 0.0322 - val_loss: 22.8630 - val_normal_output_loss: 22.8550 - val_mask_output_loss: 0.0080\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 61s 660ms/step - loss: 7.2484 - normal_output_loss: 7.2188 - mask_output_loss: 0.0295 - val_loss: 37.5192 - val_normal_output_loss: 37.5113 - val_mask_output_loss: 0.0079\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 62s 667ms/step - loss: 6.2901 - normal_output_loss: 6.2632 - mask_output_loss: 0.0269 - val_loss: 61.6536 - val_normal_output_loss: 61.6463 - val_mask_output_loss: 0.0072\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - 62s 670ms/step - loss: 5.5482 - normal_output_loss: 5.5254 - mask_output_loss: 0.0228 - val_loss: 161.2410 - val_normal_output_loss: 161.2338 - val_mask_output_loss: 0.0072\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - 61s 662ms/step - loss: 4.9047 - normal_output_loss: 4.8839 - mask_output_loss: 0.0208 - val_loss: 186.2899 - val_normal_output_loss: 186.2834 - val_mask_output_loss: 0.0064\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - 61s 660ms/step - loss: 4.3617 - normal_output_loss: 4.3431 - mask_output_loss: 0.0187 - val_loss: 344.8712 - val_normal_output_loss: 344.8651 - val_mask_output_loss: 0.0061\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 3.8856 - normal_output_loss: 3.8685 - mask_output_loss: 0.0171 - val_loss: 216.5807 - val_normal_output_loss: 216.5747 - val_mask_output_loss: 0.0060\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 3.7728 - normal_output_loss: 3.7571 - mask_output_loss: 0.0157 - val_loss: 33.3311 - val_normal_output_loss: 33.3255 - val_mask_output_loss: 0.0056\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 3.2248 - normal_output_loss: 3.2104 - mask_output_loss: 0.0144 - val_loss: 61.0447 - val_normal_output_loss: 61.0394 - val_mask_output_loss: 0.0053\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - 60s 645ms/step - loss: 2.8808 - normal_output_loss: 2.8679 - mask_output_loss: 0.0130 - val_loss: 108.1086 - val_normal_output_loss: 108.1033 - val_mask_output_loss: 0.0053\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - 60s 651ms/step - loss: 2.5894 - normal_output_loss: 2.5771 - mask_output_loss: 0.0123 - val_loss: 138.0464 - val_normal_output_loss: 138.0412 - val_mask_output_loss: 0.0052\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - 61s 656ms/step - loss: 2.3100 - normal_output_loss: 2.2984 - mask_output_loss: 0.0116 - val_loss: 65.4207 - val_normal_output_loss: 65.4154 - val_mask_output_loss: 0.0053\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - 62s 665ms/step - loss: 2.0558 - normal_output_loss: 2.0455 - mask_output_loss: 0.0103 - val_loss: 161.1308 - val_normal_output_loss: 161.1259 - val_mask_output_loss: 0.0049\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 1.8380 - normal_output_loss: 1.8285 - mask_output_loss: 0.0095 - val_loss: 148.7250 - val_normal_output_loss: 148.7202 - val_mask_output_loss: 0.0048\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 1.6545 - normal_output_loss: 1.6455 - mask_output_loss: 0.0090 - val_loss: 198.9829 - val_normal_output_loss: 198.9783 - val_mask_output_loss: 0.0047\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - 59s 642ms/step - loss: 1.5128 - normal_output_loss: 1.5043 - mask_output_loss: 0.0085 - val_loss: 305.7368 - val_normal_output_loss: 305.7323 - val_mask_output_loss: 0.0046\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - 60s 651ms/step - loss: 1.4308 - normal_output_loss: 1.4229 - mask_output_loss: 0.0079 - val_loss: 240.8547 - val_normal_output_loss: 240.8501 - val_mask_output_loss: 0.0046\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - 61s 655ms/step - loss: 1.2950 - normal_output_loss: 1.2875 - mask_output_loss: 0.0075 - val_loss: 350.4580 - val_normal_output_loss: 350.4537 - val_mask_output_loss: 0.0044\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - 62s 666ms/step - loss: 1.1801 - normal_output_loss: 1.1729 - mask_output_loss: 0.0071 - val_loss: 360.7626 - val_normal_output_loss: 360.7581 - val_mask_output_loss: 0.0045\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 1.0610 - normal_output_loss: 1.0542 - mask_output_loss: 0.0068 - val_loss: 217.1338 - val_normal_output_loss: 217.1293 - val_mask_output_loss: 0.0045\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - 60s 646ms/step - loss: 0.9708 - normal_output_loss: 0.9643 - mask_output_loss: 0.0065 - val_loss: 353.5184 - val_normal_output_loss: 353.5139 - val_mask_output_loss: 0.0046\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - 61s 661ms/step - loss: 0.9106 - normal_output_loss: 0.9040 - mask_output_loss: 0.0066 - val_loss: 616.0897 - val_normal_output_loss: 616.0852 - val_mask_output_loss: 0.0045\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - 60s 652ms/step - loss: 0.8243 - normal_output_loss: 0.8185 - mask_output_loss: 0.0059 - val_loss: 833.8444 - val_normal_output_loss: 833.8400 - val_mask_output_loss: 0.0043\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.8061 - normal_output_loss: 0.8002 - mask_output_loss: 0.0058 - val_loss: 33.2312 - val_normal_output_loss: 33.2059 - val_mask_output_loss: 0.0253\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - 60s 647ms/step - loss: 1.0753 - normal_output_loss: 1.0674 - mask_output_loss: 0.0080 - val_loss: 414.9983 - val_normal_output_loss: 414.9936 - val_mask_output_loss: 0.0046\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - 60s 645ms/step - loss: 0.8122 - normal_output_loss: 0.8063 - mask_output_loss: 0.0059 - val_loss: 220.9613 - val_normal_output_loss: 220.9572 - val_mask_output_loss: 0.0041\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 0.6954 - normal_output_loss: 0.6900 - mask_output_loss: 0.0054 - val_loss: 75.9817 - val_normal_output_loss: 75.9776 - val_mask_output_loss: 0.0041\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - 60s 651ms/step - loss: 0.6355 - normal_output_loss: 0.6305 - mask_output_loss: 0.0050 - val_loss: 80.4054 - val_normal_output_loss: 80.4014 - val_mask_output_loss: 0.0041\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - 60s 643ms/step - loss: 0.5932 - normal_output_loss: 0.5885 - mask_output_loss: 0.0047 - val_loss: 106.0940 - val_normal_output_loss: 106.0899 - val_mask_output_loss: 0.0040\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.5580 - normal_output_loss: 0.5534 - mask_output_loss: 0.0045 - val_loss: 150.7096 - val_normal_output_loss: 150.7057 - val_mask_output_loss: 0.0040\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 0.5291 - normal_output_loss: 0.5248 - mask_output_loss: 0.0043 - val_loss: 183.4527 - val_normal_output_loss: 183.4487 - val_mask_output_loss: 0.0040\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - 61s 655ms/step - loss: 0.5085 - normal_output_loss: 0.5042 - mask_output_loss: 0.0043 - val_loss: 171.5766 - val_normal_output_loss: 171.5728 - val_mask_output_loss: 0.0038\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 0.4934 - normal_output_loss: 0.4893 - mask_output_loss: 0.0042 - val_loss: 262.5394 - val_normal_output_loss: 262.5355 - val_mask_output_loss: 0.0039\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 0.5146 - normal_output_loss: 0.5106 - mask_output_loss: 0.0040 - val_loss: 361.1280 - val_normal_output_loss: 361.1240 - val_mask_output_loss: 0.0040\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.5613 - normal_output_loss: 0.5574 - mask_output_loss: 0.0038 - val_loss: 426.4981 - val_normal_output_loss: 426.4940 - val_mask_output_loss: 0.0040\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - 59s 638ms/step - loss: 0.4549 - normal_output_loss: 0.4512 - mask_output_loss: 0.0037 - val_loss: 350.5815 - val_normal_output_loss: 350.5775 - val_mask_output_loss: 0.0040\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.4398 - normal_output_loss: 0.4362 - mask_output_loss: 0.0036 - val_loss: 404.0818 - val_normal_output_loss: 404.0777 - val_mask_output_loss: 0.0040\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - 60s 647ms/step - loss: 0.4281 - normal_output_loss: 0.4245 - mask_output_loss: 0.0036 - val_loss: 465.1519 - val_normal_output_loss: 465.1479 - val_mask_output_loss: 0.0039\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - 61s 658ms/step - loss: 0.4176 - normal_output_loss: 0.4139 - mask_output_loss: 0.0037 - val_loss: 586.1523 - val_normal_output_loss: 586.1486 - val_mask_output_loss: 0.0038\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 0.4105 - normal_output_loss: 0.4072 - mask_output_loss: 0.0034 - val_loss: 634.1736 - val_normal_output_loss: 634.1699 - val_mask_output_loss: 0.0037\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 0.4084 - normal_output_loss: 0.4051 - mask_output_loss: 0.0033 - val_loss: 623.3268 - val_normal_output_loss: 623.3231 - val_mask_output_loss: 0.0037\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.3982 - normal_output_loss: 0.3950 - mask_output_loss: 0.0032 - val_loss: 886.5627 - val_normal_output_loss: 886.5591 - val_mask_output_loss: 0.0036\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.3962 - normal_output_loss: 0.3931 - mask_output_loss: 0.0031 - val_loss: 1095.1636 - val_normal_output_loss: 1095.1598 - val_mask_output_loss: 0.0036\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - 60s 646ms/step - loss: 0.3964 - normal_output_loss: 0.3934 - mask_output_loss: 0.0030 - val_loss: 1100.7679 - val_normal_output_loss: 1100.7645 - val_mask_output_loss: 0.0036\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - 58s 622ms/step - loss: 0.5456 - normal_output_loss: 0.5413 - mask_output_loss: 0.0042 - val_loss: 678.3381 - val_normal_output_loss: 678.3342 - val_mask_output_loss: 0.0040\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 0.4366 - normal_output_loss: 0.4331 - mask_output_loss: 0.0035 - val_loss: 256.7646 - val_normal_output_loss: 256.7611 - val_mask_output_loss: 0.0035\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 0.4001 - normal_output_loss: 0.3968 - mask_output_loss: 0.0033 - val_loss: 279.7401 - val_normal_output_loss: 279.7365 - val_mask_output_loss: 0.0034\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - 59s 635ms/step - loss: 0.3850 - normal_output_loss: 0.3818 - mask_output_loss: 0.0031 - val_loss: 428.5655 - val_normal_output_loss: 428.5620 - val_mask_output_loss: 0.0035\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - 59s 635ms/step - loss: 0.3751 - normal_output_loss: 0.3721 - mask_output_loss: 0.0030 - val_loss: 632.6443 - val_normal_output_loss: 632.6409 - val_mask_output_loss: 0.0035\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - 60s 645ms/step - loss: 0.3661 - normal_output_loss: 0.3632 - mask_output_loss: 0.0029 - val_loss: 789.7398 - val_normal_output_loss: 789.7363 - val_mask_output_loss: 0.0035\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - 58s 629ms/step - loss: 0.3590 - normal_output_loss: 0.3562 - mask_output_loss: 0.0028 - val_loss: 954.1007 - val_normal_output_loss: 954.0972 - val_mask_output_loss: 0.0034\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.3533 - normal_output_loss: 0.3506 - mask_output_loss: 0.0027 - val_loss: 1038.3575 - val_normal_output_loss: 1038.3542 - val_mask_output_loss: 0.0034\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 0.3488 - normal_output_loss: 0.3461 - mask_output_loss: 0.0027 - val_loss: 1179.2201 - val_normal_output_loss: 1179.2167 - val_mask_output_loss: 0.0034\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - 60s 645ms/step - loss: 0.3488 - normal_output_loss: 0.3462 - mask_output_loss: 0.0026 - val_loss: 1012.8994 - val_normal_output_loss: 1012.8960 - val_mask_output_loss: 0.0034\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - 59s 633ms/step - loss: 0.3624 - normal_output_loss: 0.3599 - mask_output_loss: 0.0025 - val_loss: 920.5441 - val_normal_output_loss: 920.5407 - val_mask_output_loss: 0.0033\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 0.3809 - normal_output_loss: 0.3785 - mask_output_loss: 0.0025 - val_loss: 1600.3290 - val_normal_output_loss: 1600.3256 - val_mask_output_loss: 0.0034\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 0.3678 - normal_output_loss: 0.3655 - mask_output_loss: 0.0024 - val_loss: 1045.2491 - val_normal_output_loss: 1045.2460 - val_mask_output_loss: 0.0034\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.3576 - normal_output_loss: 0.3553 - mask_output_loss: 0.0022 - val_loss: 1301.3964 - val_normal_output_loss: 1301.3929 - val_mask_output_loss: 0.0035\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - 61s 654ms/step - loss: 0.3545 - normal_output_loss: 0.3523 - mask_output_loss: 0.0022 - val_loss: 2238.2610 - val_normal_output_loss: 2238.2578 - val_mask_output_loss: 0.0034\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 0.3516 - normal_output_loss: 0.3494 - mask_output_loss: 0.0022 - val_loss: 2124.3423 - val_normal_output_loss: 2124.3391 - val_mask_output_loss: 0.0034\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - 60s 643ms/step - loss: 0.3493 - normal_output_loss: 0.3472 - mask_output_loss: 0.0021 - val_loss: 1851.6110 - val_normal_output_loss: 1851.6074 - val_mask_output_loss: 0.0036\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 0.3566 - normal_output_loss: 0.3544 - mask_output_loss: 0.0022 - val_loss: 1688.6178 - val_normal_output_loss: 1688.6139 - val_mask_output_loss: 0.0034\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.3670 - normal_output_loss: 0.3647 - mask_output_loss: 0.0023 - val_loss: 1813.6190 - val_normal_output_loss: 1813.6152 - val_mask_output_loss: 0.0035\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - 57s 618ms/step - loss: 0.3856 - normal_output_loss: 0.3835 - mask_output_loss: 0.0021 - val_loss: 1654.3073 - val_normal_output_loss: 1654.3033 - val_mask_output_loss: 0.0036\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - 58s 629ms/step - loss: 0.4129 - normal_output_loss: 0.4109 - mask_output_loss: 0.0020 - val_loss: 2143.5583 - val_normal_output_loss: 2143.5544 - val_mask_output_loss: 0.0037\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 0.3865 - normal_output_loss: 0.3845 - mask_output_loss: 0.0020 - val_loss: 1897.8983 - val_normal_output_loss: 1897.8949 - val_mask_output_loss: 0.0036\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 0.3704 - normal_output_loss: 0.3685 - mask_output_loss: 0.0019 - val_loss: 1868.1136 - val_normal_output_loss: 1868.1106 - val_mask_output_loss: 0.0035\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.3587 - normal_output_loss: 0.3567 - mask_output_loss: 0.0020 - val_loss: 2951.4146 - val_normal_output_loss: 2951.4109 - val_mask_output_loss: 0.0034\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - 58s 626ms/step - loss: 0.3520 - normal_output_loss: 0.3500 - mask_output_loss: 0.0020 - val_loss: 2945.5859 - val_normal_output_loss: 2945.5823 - val_mask_output_loss: 0.0037\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - 60s 652ms/step - loss: 0.3467 - normal_output_loss: 0.3447 - mask_output_loss: 0.0019 - val_loss: 2973.8855 - val_normal_output_loss: 2973.8833 - val_mask_output_loss: 0.0034\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.3369 - normal_output_loss: 0.3350 - mask_output_loss: 0.0018 - val_loss: 1944.3264 - val_normal_output_loss: 1944.3225 - val_mask_output_loss: 0.0036\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - 59s 639ms/step - loss: 0.3325 - normal_output_loss: 0.3307 - mask_output_loss: 0.0018 - val_loss: 2341.2634 - val_normal_output_loss: 2341.2600 - val_mask_output_loss: 0.0035\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - 59s 635ms/step - loss: 0.3289 - normal_output_loss: 0.3272 - mask_output_loss: 0.0018 - val_loss: 4081.3430 - val_normal_output_loss: 4081.3391 - val_mask_output_loss: 0.0037\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.3237 - normal_output_loss: 0.3219 - mask_output_loss: 0.0018 - val_loss: 3802.7771 - val_normal_output_loss: 3802.7732 - val_mask_output_loss: 0.0037\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - 59s 632ms/step - loss: 0.3219 - normal_output_loss: 0.3201 - mask_output_loss: 0.0018 - val_loss: 2718.0527 - val_normal_output_loss: 2718.0496 - val_mask_output_loss: 0.0037\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - 59s 635ms/step - loss: 0.3207 - normal_output_loss: 0.3191 - mask_output_loss: 0.0017 - val_loss: 2721.3430 - val_normal_output_loss: 2721.3398 - val_mask_output_loss: 0.0037\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - 58s 624ms/step - loss: 0.3205 - normal_output_loss: 0.3188 - mask_output_loss: 0.0017 - val_loss: 1204.9799 - val_normal_output_loss: 1204.9761 - val_mask_output_loss: 0.0038\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.3173 - normal_output_loss: 0.3157 - mask_output_loss: 0.0016 - val_loss: 903.0759 - val_normal_output_loss: 903.0721 - val_mask_output_loss: 0.0038\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 0.3201 - normal_output_loss: 0.3185 - mask_output_loss: 0.0016 - val_loss: 921.4407 - val_normal_output_loss: 921.4370 - val_mask_output_loss: 0.0036\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - 60s 644ms/step - loss: 0.3251 - normal_output_loss: 0.3235 - mask_output_loss: 0.0016 - val_loss: 1629.4659 - val_normal_output_loss: 1629.4623 - val_mask_output_loss: 0.0038\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - 59s 646ms/step - loss: 0.3360 - normal_output_loss: 0.3345 - mask_output_loss: 0.0015 - val_loss: 1885.3655 - val_normal_output_loss: 1885.3616 - val_mask_output_loss: 0.0037\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - 58s 626ms/step - loss: 0.3444 - normal_output_loss: 0.3429 - mask_output_loss: 0.0015 - val_loss: 2633.9983 - val_normal_output_loss: 2633.9949 - val_mask_output_loss: 0.0038\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - 58s 631ms/step - loss: 0.3391 - normal_output_loss: 0.3377 - mask_output_loss: 0.0015 - val_loss: 1866.7256 - val_normal_output_loss: 1866.7217 - val_mask_output_loss: 0.0037\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.3343 - normal_output_loss: 0.3328 - mask_output_loss: 0.0015 - val_loss: 2456.8650 - val_normal_output_loss: 2456.8616 - val_mask_output_loss: 0.0037\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - 59s 641ms/step - loss: 0.3384 - normal_output_loss: 0.3369 - mask_output_loss: 0.0014 - val_loss: 2688.7527 - val_normal_output_loss: 2688.7488 - val_mask_output_loss: 0.0039\n",
            "Sat Nov 27 12:46:58 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d45a71b"
      },
      "source": [
        "print(start, end)\n",
        "print(\"Starting time: \", time.ctime(start))\n",
        "print(\"Ending time: \", time.ctime(end))\n",
        "print(\"Time elapsed: \", datetime.timedelta(seconds=round(end - start)))\n",
        "print('')\n",
        "print(\"Number of parameters: \", model.count_params())\n",
        "print(\"\")\n",
        "model.save(model_path + '/' + model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83c5e4dc"
      },
      "source": [
        "model.summary()\n",
        "model.count_params()\n",
        "#tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
